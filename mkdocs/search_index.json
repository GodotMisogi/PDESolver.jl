{
    "docs": [
        {
            "location": "/", 
            "text": "PDESolver Documentation\n\n\nWelcome to the PDESolver documentation.  These documents provide an overview of PDESolver, a Julia based solver for partial differential equations. This page will describe the form of the equations and the Summation-By-Parts operators used to discretize them.  The \nTable of Contents\n links to the components of PDESolver that calculate each term.\n\n\n\n\nForm of the Equation\n\n\nPDESolver discretizes equation in the form:\n\n\n$\\frac{n!}{k!(n - k)!} = \\binom{n}{k}$\n\n\n$\\frac{\\partial q}{\\partial t} = \\mathcal{R}(u, t)$\n\n\n\n\nTable of Contents\n\n\n\n\nOverview of Physics Modules\n\n\nAdvection Physics Documentation\n\n\nEuler Physics Documentation\n\n\nSimple ODE Documentation\n\n\nNonlinearSolvers Documentation\n\n\nInput Module Documentation\n\n\nUtilties", 
            "title": "Home"
        }, 
        {
            "location": "/#pdesolver-documentation", 
            "text": "Welcome to the PDESolver documentation.  These documents provide an overview of PDESolver, a Julia based solver for partial differential equations. This page will describe the form of the equations and the Summation-By-Parts operators used to discretize them.  The  Table of Contents  links to the components of PDESolver that calculate each term.", 
            "title": "PDESolver Documentation"
        }, 
        {
            "location": "/#form-of-the-equation", 
            "text": "PDESolver discretizes equation in the form:  $\\frac{n!}{k!(n - k)!} = \\binom{n}{k}$  $\\frac{\\partial q}{\\partial t} = \\mathcal{R}(u, t)$", 
            "title": "Form of the Equation"
        }, 
        {
            "location": "/#table-of-contents", 
            "text": "Overview of Physics Modules  Advection Physics Documentation  Euler Physics Documentation  Simple ODE Documentation  NonlinearSolvers Documentation  Input Module Documentation  Utilties", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/interfaces/", 
            "text": "Interfaces in PDESolver\n\n\nPDESolver depends on the the three main objects, the \nAbstractSolutionData\n object,  \nAbstractMesh\n object, and the SBP object implementing certain interfaces. This document describes what the interfaces are, and gives some hints for how to  implement them.\n\n\nBefore doing so, a short description of what general Julia interfaces look like is in order.   The paradigm of Julia code is that of \"objects with associated functions\", where  a new Type is defined, and then functions that take the Type as an argument are defined. The functions define the interface to the Type. The Type holds data (ie. state), and the functions perform operations on that state (ie. behavior). Perhaps counter-intuitively, it is generally not recommended for users of a type to access the fields directly. Instead, any needed operations on the data that the Type holds should be provided through functions. The benefit of this convention is that it imposes no requirements on how the Type stores its data or implements its behavior. This is important because a user of the Type should not be concerned with these things. The user needs to know what behavior the Type has, but not how it is implemented. This distinction becomes even more important when there are multiple implementations certain functionality. The user should be able to seamlessly transition between different implementations. This requires all implementations have the same interface.\n\n\nThe question of how to enforce interfaces, and how strongly to do so, is still an open question in Julia. Some relevant Github issues: * \n5\n * \n4935\n * \n6975\n\n\nOne of the strongest arguments against the \"functions as interfaces\" idea is that for many applications, just storing data in an array is best. Creating interface functions for the Type to implement the array interface would be a lot of extra code with no benefit. For this reason, it makes sense to directly access the fields of some Types, to avoid trivial get/set methods. We do this extensively in PDESolver, because arrays are the natural choice for storing the kind of data used in PDESolver.\n\n\n\n\nAbstractSolutionData\n\n\nODLCommonTools defines:\n\n\nabstract AbstractSolutionData{Tsol, Tres}\n.\n\n\nThe purpose of an \nAbstractSolutionData\n is to hold all the data related to the solution of an equation. This includes the solution at every node and any auxiliary quantities. The storage for any quantity that is calculated over the entire mesh should be allocated as part of this object, in order to avoid repeatedly reallocated the array for every residual evaluation. In general, there should never be a need to allocate a vector longer than the number of degrees of freedom at a node (or a matrix similarly sized matrix) during a residual evaluation. Structuring code such that it conforms with this requirement has significant performance benefits because it reduces memory allocation/deallocation.\n\n\nThe static parameter \nTsol\n is the datatype of the solution variables and \nTres\n is the datatype of the residual (when computing the Jacobian with finite differences or algorithmic differentiation, these will be the same).\n\n\n\n\nRequired Fields\n\n\nThe required fields of an \nAbstractSolutionData\n are:\n\n\n  q::AbstractArray{Tsol, 3}\n  q_vec::AbstractArray{Tsol, 1}\n  shared_data::AbstractArray{SharedFaceData{Tsol}, 1}\n  res::AbstractArray{Tres, 3}\n  res_vec::AbstractArray{Tres, 1}\n  M::AbstractArray{Float64, 1}\n  Minv::AbstractArray{Float64, 1}\n  disassembleSolution::Function\n  assembleSolution::Function\n  multiplyA0inv::Function\n  majorIterationCallback::Function\n  params{Tsol..., Tdim}::AbstractParamType{Tdim}\n\n\n\n\nThe purpose of these fields are:\n\n\nq\n: to hold the solution variables in an element-based array.      This array should be \nnumDofPerNode\n x \nnumNodesPerElement\n x \nnumEl\n.      The residual evaluation \nonly\n uses \nq\n, never \nq_vec\n\n\nq_vec\n: to hold the solution variables as a vector, used for any linear algebra operations and time stepping. This array should have a length equal to the total number of degrees of freedom in the mesh. Even though this vector is not used by the residual evaluation, it is needed for many other operations, so it is allocated here so the memory can be reused. There are functions to facilitate the scattering of values from \nq_vec\n to \nq\n. Note that for Continuous Galerkin type discretization (as opposed to Discontinuous Galerkin discretizations), there is not a corresponding \"gather\" operation (ie. \nq\n -\n \nq_vec\n).\n\n\nshared_data\n is a vector of length \nnpeers\n.  Each element contains the data               needed send and receive the \nq\n variables to/from other               the corresponding MPI rank listed in \nmesh.peer_parts\n.               The precise contents of \nSharedFaceData\n is documented in the               \nUtils\n module, however they do include the send and receive               buffers.\n\n\nres\n: similar to \nq\n, except that the residual evaluation function populates        it with the residual values.          As with \nq\n, the residual evaluation function only interacts with this array,        never with \nres_vec\n.\n\n\nres_vec\n: similar to \nq_vec\n.  Unlike \nq_vec\n there are functions to perform an            additive reduction (basically a \"gather\") of \nres\n to \nres_vec\n.              For continuous Galerkin discretizations, the corresponding \"scatter\"            (ie. \nres_vec\n -\n res`) may not exist.\n\n\nM\n:  The mass matrix of the entire mesh.  Because SBP operators have diagonal       mass matrices, this is a vector.  Length numDofPerNode x numNodes (where       numNodes is the number of nodes in the entire mesh).\n\n\nMinv\n:  The inverse of the mass matrix.\n\n\ndisassembleSolution\n:  Function that takes the a vector such as \nq_vec\n and                         scatters it to an array such as \nq\n.                         This function must have the signature:                         \ndisassembleSolution(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, q_arr:AbstractArray{T, 3}, q_vec::AbstractArray{T, 1})\n                         Because this variable is a field of a type, it will be dynamically dispatched.                         Although this is slower than compile-time dispatch, the cost is insignificant compared to the cost of evaluating the residual, so the added flexibility of having this function as a field is worth the cost.\n\n\nassembleSolution\n:  Function that takes an array such as \nres\n and performs an additive reduction to a vector such as \nres_vec\n.                      This function must have the signature:                      \nassembleSolution(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, res_arr::AbstractArray{T, 3}, res_vec::AbstractArray{T, 1}, zero_resvec=true)\n                      The argument \nzero_resvec\n determines whether \nres_vec\n is zeroed before the reduction is performed.                      Because it is an additive reduction, elements of the vector are only added to, never overwritten, so forgetting to zero out the vector could cause strange results.                      Thus the default is true.\n\n\nmultiplyA0inv\n:  Multiplies the solution values at each node in an array such as \nres\n by the inverse of the coefficient matrix of the time term of the equation.                   This function is used by time marching methods.                   For some equations, this matrix is the identity matrix, so it can be a no-op, while for others might not be.                   The function must have the signature:\n\n\nmultiplyA0inv(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, res_arr::AbstractArray{Tsol, 3})\n\n\nmajorIterationCallback\n:  function called before every step of Newton's method or stage of an explicit time marching scheme. This function is used to do output and logging. The function must have the signature:\n\n\nfunction majorIterationCallback(itr, mesh::AbstractMesh, sbp::AbstractSBP, eqn::AbstractEulerData, opts)\n\n\nparams\n:  user defined type that inherits from \nAbstractParamType\n. The purpose of this type is to store any variables that need to be quickly accessed or updated. The only required fields are: * \nt::Float64\n: hold the current time value * \norder\n: order of accuracy of the discretization (same as \nAbstractMesh.order\n) *  \ntime::Timings\n: an object to record how long different parts of the code take,   defined in the Utils module.\n\n\n\n\nAbstractMesh\n\n\nODLCommonTools defines:\n\n\nabstract AbstractMesh{Tmsh}\n.\n\n\nThe purpose of an \nAbstractMesh\n is to hold all the mesh related data that the  solver will need.  It also serves to establish an interface between the solver  and whatever mesh software is used.  By storing all data in the fields of the  \nAbstractMesh\n object, the details of how the mesh software stores and allows  retrieval of data are not needed by the solver.  This should make it easy to  accommodate different mesh software without making any changes to the solver.\n\n\nThe static parameter \nTmsh\n is used to enable differentiation with respect to the mesh variable in the future.\n\n\n\n\nRequired Fields\n\n\n  # counts\n  numVert::Integer\n  numEl::Integer\n  numNodes::Integer\n  numDof::Integer\n  numDofPerNode::Integer\n  numNodesPerElement::Integer\n  order::Integer\n  numNodesPerFace::Int\n\n  # parallel counts\n  npeers::Int\n  numGlobalEl::Int\n  numSharedEl::Int\n  peer_face_counts::Array{Int, 1}\n  local_element_counts::Array{Int, 1}\n  remote_element_counts::array{Int, 1}\n\n  # MPI Info\n  comm::MPI.Comm\n  myrank::Int\n  commsize::Int\n  peer_parts::Array{Int, 1}\n\n  # Discretization type\n  isDG::Bool\n  isInterpolated::bool\n\n  # mesh data\n  coords::AbstractArray{Tmsh, 3}\n  dxidx::AbstractArray{Tmsh, 4}\n  jac::AbstractArray{Tmsh, 2}\n\n  # interpolated data\n  coords_bndry::Array{Tmsh, 3}\n  dxidx_bndry::Array{Tmsh, 4}\n  jac_bndry::Array{T1, 2}\n  dxidx_face::Array{Tmsh, 4}\n  jac_face::Array{Tmsh, 2}\n\n  # parallel data\n  coords_sharedface::Array{Array{Tmsh, 3}, 1}\n  dxidx_sharedface::Array{Array{Tmsh, 4}, 1}\n  jac_sharedface::Array{Array{Tmsh, 2}, 1}  \n\n  # boundary condition data\n  numBC::Integer\n  numBoundaryEdges::Integer\n  bndryfaces::AbstractArray{Boundary, 1}\n  bndry_offsets::AbstractArray{Integer, 1}\n  bndry_funcs::AbstractArray{BCType, 1}\n\n  # interior edge data\n  numInterfaces::Integer\n  interfaces::AbstractArray{Interface, 1}\n\n  # degree of freedom number data\n  dofs::AbstractArray{Integer, 2}\n  dof_offset::Int\n  sparsity_bnds::AbstractArray{Integer, 2}\n  sparsity_nodebnds::AbstractArray{Integer, 2}\n\n  # mesh coloring data\n  numColors::Integer\n  maxColors::Integer\n  color_masks::AbstractArray{ AbstractArray{Number, 1}, 1}\n  shared_element_colormasks::Array{Array{BitArray{1}, 1}, 1}\n  pertNeighborEls::AbstractArray{Integer, 2}\n\n  # parallel bookkeeping\n  bndries_local::Array{Array{Boundary, 1}, 1}\n  bndries_remote::Array{Array{Boundary, 1}, 1}\n  shared_interfaces::Array{Array{Interface, 1}, 1}\n  shared_element_offsets::Array{Int, 1}\n  local_element_lists::Array{Array{Int, 1}, 1}\n\n\n\n\n\n\n\nCounts\n\n\nnumVert\n:  number of vertices in the mesh\n\n\nnumEl\n:  number of elements in the mesh\n\n\nnumNodes\n: number of nodes in the mesh\n\n\nnumDof\n:  number of degrees of freedom in the mesh (= \nnumNodes\n * \nnumDofPerNode\n)\n\n\nnumDofPerNode\n:  number of degrees of freedom on each node.\n\n\nnumNodesPerElement\n:  number of nodes on each element.\n\n\norder\n:  order of the discretization (ie. first order, second order...), where           an order \np\n discretization should have a convergence rate of \np+1\n.\n\n\nnumNodesPerFace\n: number of nodes on an edge in 2D or face in 3D.  For interpolated                    meshes it is the number of interpolation points on the face\n\n\n\n\nParallel Counts\n\n\nnpeers\n: number of processes that have elements that share a face with the current          process\n\n\nnumGlobalEl\n: number of locally owned elements + number of non-local elements that                share a face with a locally owned element\n\n\nnumSharedEl\n: number of non-local elements that share a face with a locally owned                element\n\n\nlocal_element_counts\n: array of length \nnpeers\n, number of local elements                         that share a face with each remote process\n\n\nremote_element_counts\n: array of length \nnpeers\n, number of remote elements                          that share a face with with current process\n\n\n\n\nMPI Info\n\n\ncomm\n: the MPI Communicator the mesh is defined on \nmyrank\n: rank of current MPI process (0-based) \ncommsize\n: number of MPI processes on this communicator \npeer_parts\n: array of MPI proccess ranks for each process that has elements that               share a face with the current process\n\n\n\n\nDiscretization Type\n\n\nisDG\n: true if mesh is a DG type discretization \nisInterpolated\n: true if mesh requires data to be interpolated to the faces                   of an element\n\n\n\n\nMesh Data\n\n\ncoords\n: \nn\n x \nnumNodesPerElement\n x \nnumEl\n array, where \nn\n is the dimensionality of            the equation being solved (2D or 3D typically).              \ncoords[:, nodenum, elnum] = [x, y, z]\n coordinates of node \nnodenum\n            of element \nelnum\n.\n\n\ndxidx\n:  \nn\n x \nn\n x \nnumNodesPerElement\n x \nnumEl\n, where \nn\n is defined above. It stores the mapping jacobian scaled by \n( 1/det(jac) dxi/dx )\n where \nxi\n are the parametric coordinates, \nx\n are the physical (x,y,z) coordinates, and \njac\n is the determinant of the mapping jacobian \ndxi/ dx\n.\n\n\njac\n  : \nnumNodesPerElement\n x \nnumEl\n array, holding the determinant of the          mapping jacobian \ndxi/dx\n at each node of each element.\n\n\n\n\nInterpolated Data\n\n\nThis data is used for interpolated mesh only.\n\n\ncoords_bndry\n: coordinates of nodes on the boundary of the mesh,                 2 x \nnumFaceNodes\n x \nnumBoundaryEdges\n\n\ndxidx_bndry\n: 2 x 2 x \nnumFaceNodes\n x \nnumBoundary edges array of\ndxidx`                interpolated to the boundary of the mesh\n\n\njac_bndry\n: \nnumFaceNodes\n x \nnumBoundaryEdges\n array of \njac\n interpolated               to the boundary\n\n\ndxidx_face\n: 2 x 2 x \nnumFaceNodes\n x \nnumInterfaces\n array of \ndxidx\n               interpolated to the face shared between two elements\n\n\njac_face\n: \nnumNodesPerFace\n x \nnumInterfaces\n array of \njac\n interpolated              to the face shared between two element\n\n\n\n\nParallel Data\n\n\nThis data is required for parallelizing interpolated DG meshes\n\n\ncoords_sharedface\n: array of arrays, one array for each peer process,                      containing the coordinates of the nodes on the faces                      shared between a local element on a non-local element.                      Each array is 2 x \nnumFaceNodes\n x number of faces shared                      with this process. \ndxidx_sharedface\n: similar to \ncoords_sharedface\n, \ndxidx\n interpolated to                     faces between elements in different processes, each                     array is 2 x 2 x \nnumFaceNodes\n x number of faces shared                     with this process. \njac_sharedface\n: similar to \ncoords_sharedface\n, \njac\n interpolated to faces                   between a local element and a non-local element. Each array                   is \nnumFaceNodes\n x number of faces shared with this process.\n\n\n\n\nBoundary Condition Data\n\n\nThe mesh object stores data related to applying boundary conditions. Boundary conditions are imposed weakly, so there is no need to remove degrees of freedom from the mesh when Dirichlet boundary conditions are applied. In order to accommodate any combination of boundary conditions, an array of functors are stored as part of the mesh object, along with lists of which mesh edges (or faces in 3D) should have which boundary condition applied to them\n\n\nnumBC\n: number of different types of boundary conditions used.\n\n\nnumBoundaryEdges\n: number of mesh edges that have boundary conditions applied                     to them.\n\n\nbndryfaces\n:  array of Boundary objects (which contain the element number and                the local index of the edge), of length \nnumBoundaryEdges\n.\n\n\nbndry_offsets\n:  array of length numBC+1, where \nbndry_offsets[i]\n is the index                   in \nbndryfaces\n where the edges that have boundary condition                   \ni\n applied to them start.                   The final entry in \nbndry_offsets\n should be \nnumBoundaryEdges + 1\n.                   Thus \nbndryfaces[ bndry_offsets[i]:(bndry_offsets[i+1] - 1) ]\n                   contains all the boundary edges that have boundary condition                   \ni\n applied to them.\n\n\nbndry_funcs\n:  array of boundary functors, length \nnumBC\n.  All boundary                 functors are subtypes of \nBCType\n.  Because \nBCType\n is an                 abstract type, the elements of this array should not be used                 directly, but passed as an argument to another function, to                  avoid type instability.\n\n\n\n\nInterior Edge Data\n\n\nData about interior mesh edges (or faces in 3D) is stored to enable use of edge stabilization or Discontinuous Galerkin type discretizations. Only data for edges (faces) that are shared by two elements are stored (ie. boundary edges are not considered).\n\n\nnumInterfaces\n:  number of interior edges\n\n\ninterfaces\n:  array of Interface types (which contain the element numbers for                the two elements sharing the edge, and the local index of the                edge from the perspective of the two elements, and an indication                of the relative edge orientation).                The two element are referred to as \nelementL\n and \nelementR\n,                but the choice of which element is \nelementL\n and which is                \nelementR\n is arbitrary.                The length of the array is numInterfaces.               Unlike \nbndryfaces\n, the entries in the array do not have to be in               any particular order.\n\n\n\n\nDegree of Freedom Numbering Data\n\n\ndofs\n:  \nnumDofPerNode\n x \nnumNodesPerElement\n x \nnumEl\n array. Holds the local degree of freedom number of each degree of freedom. Although the exact method used to assign dof numbers is not critical, all degrees of freedom on a node must be numbered sequentially.\n\n\ndof_offset\n: offset added to the local dof number to make it a global dof               number.\n\n\nsparsity_bnds\n:  2 x \nnumDof\n array. \nsparsity_bnds[:, i]\n holds the maximum, minimum degree of freedom numbers associated with degree of freedom \ni\n. In this context, degrees of freedom \ni\n and \nj\n are associated if entry \n(i,j)\n of the jacobian is non-zero. In actuality, \nsparsity_bnds\n need only define upper and lower bounds for degree of freedom associations (ie. they need not be tight bounds). This array is used to to define the sparsity pattern of the jacobian matrix.\n\n\nsparsity_nodebnds\n:  2 x numNodes array. \nsparsity_bnds[:, i]\n holds the maximum, minimum node associated with node \ni\n, similar the information stored in \nsparsity_bnds\n for degrees of freedom.\n\n\n\n\nMesh Coloring Data\n\n\nThe NonlinearSolvers module uses algorithmic differentiation to compute the Jacobian. Doing so efficiently requires perturbing multiple degrees of freedom simultaneously, but perturbing associated degrees of freedom at the same time leads to incorrect results. Mesh coloring assigns each element of the mesh to a group (color) such that every degree of freedom on each element is not associated with any other degree  of freedom on any other element of the same color. An important aspect of satisfying this condition is the use of the  element-based arrays (all arrays that store data for a quantity over the entire  mesh are \nncomp\n x \nnumNodesPerElement\n x \nnumEl\n). In such an array, any node that is part of 2 or more elements has one entry for each element. When performing algorithmic differentiation, this enables perturbing a degree of  freedom on one element without perturbing it on the other elements that share  the degree of freedom.\n\n\nFor example, consider a node that is shared by two elements. Let us say it is node 2 of element 1 and node 3 of element 2. This means \nAbstractSolutionData.q[:, 2, 1]\n stores the solution variables for this node on the first element, and \nAbstractSolutionData.q[:, 3, 2]\n stores the solution variables for the second element. Because these are different entries in the array \nAbstractSolutionData.q\n, they can be perturbed independently. Because \nAbstractSolutionData.res\n has the same format, the perturbations to \nAbstractSolutionData.q[:, 2, 1]\n are mapped to \nAbstractSolutionData.res[:, 2, 1]\n for a typical continuous Galerkin type discretization. This is a direct result of having an element-based discretization.\n\n\nThere are some discretizations, however, that are not strictly element-based. Edge stabilization, for example, causes all the degrees of freedom of one element to be associated with any elements it shares an edge with. To deal with this, we use the idea of a distance-n coloring. A distance-n coloring is a coloring where there are n elements in between two element of the same color. For element-based discretizations with element-based arrays, every element in the mesh can be the same color.  This is a distance-0 coloring. For an edge stabilization discretization, a distance-1 coloring is required, where every element is a different color than any neighbors it shares and edge with. (As a side node, the algorithms that perform a distance-1 coloring are rather complicated, so in practice we use a distance-2 coloring instead).\n\n\nIn order to do algorithmic differentiation, the \nAbstractMesh\n object must store the information that determines which elements are perturbed for which colors, and, for the edge stabilization case, how to relate a perturbation in the output  \nAbstractSolutionData.res\n to the degree of freedom in \nAbstractSolutionData.q\n in O(1) time. Each degree of freedom on an element is perturbed independently of the other degrees of freedom on the element, so the total number of residual evaluations is the number of colors times the number of degrees of freedom on an element.\n\n\nThe fields required are:\n\n\nnumColors\n:  The number of colors in the mesh. \nmaxColors\n: the maximum number of colors on any process\n\n\ncolor_masks\n:  array of length \nnumColors\n.  Each entry in the array is itself                 an array of length \nnumEl\n.  Each entry of the inner array is                 either a 1 or a 0, indicating if the current element is                 perturbed or not for the current color. For example, in \ncolor_mask_i = color_masks[i]; mask_elj = color_mask_i[j]\n, the variable \nmask_elj\n is either a 1 or a zero, determining whether or not element \nj\n is perturbed as part of color \ni\n.\n\n\nshared_element_colormasks\n: array of BitArrays controlling when to perturb                              non-local elements.  There are \nnpeers\n arrays,                              each of length number of non-local elements shared                              with this process\n\n\npertNeighborEls\n:  \nnumEl\n x \nnumColors\n array.  \nneighbor_nums[i,j]\n is the element number of of the element whose perturbation is affected element \ni\n when color \nj\n is being perturbed, or zero if element \ni\n is not affected by any  perturbation.  \n\n\n\n\nParallel Bookkeeping\n\n\nbndries_local\n: array of arrays of \nBoundary\ns describing faces shared                  with non-local elements from the local side (ie. the                  local element number and face).  The number of arrays is                  \nnpeers\n.\n\n\nbndries_remote\n: similar to \nbndries_local\n, except describing the faces                   from the non-local side.  Note that the element numbers are                   from the \nremote\n process\n\n\nshared_interfaces\n: array of arrays of \nInterface\ns describing the faces                      shared between local and non-local elements.  The local                      element is \nalways\n \nelementL\n.  The remote element is                      assigned a local number greater than \nnumEl\n.\n\n\nshared_element_offsets\n: array of length npeers+1 that contains the first                           element number assigned to elements on the shared                           interface.  The last entry is equal to the highest                           element number on the last peer process + 1.  The                           elements numbers assigned to a given peer must form                           a contiguous range.\n\n\nlocal_element_lists\n: array of arrays containing the element numbers of the                        elements that share a face with each peer process\n\n\n\n\nOther Functions\n\n\nThe mesh module must also defines and exports the functions\n\n\nsaveSolutionToMesh(mesh::MeshImplementationType, vec::AbstractVector)\nwriteVisFiles(mesh::MeshImplementationType, fname::ASCIIString)\n\n\n\n\nwhere the first function takes a vector of length \nnumDof\n and saves it to the mesh, and the second writes Paraview files for the mesh, including the solution field.\n\n\n\n\nPhysics Module\n\n\nFor every new physics to be solved, a new module should be created. The purpose of this module is to evaluate the equation:\n\n\nM dq/dt = f(q)\n\n\nwhere \nM\n is the mass matrix. For steady problems, \ndq/dt = 0\n and the module evaluates the residual. For unsteady problems, the form \nM dq/dt = f(q)\n is suitable for explicit time marching.\n\n\nEvery physics module must define a boundary condition called \ndefaultBC\n.  If the user does not specify a boundary condition on any geometric edges, the mesh constructor will add a new boundary condition and assign all mesh edges classified on the unspecified geometric edges are assigned to it.  This boundary condition can be a no-op if that is correct for the physics.\n\n\n\n\nInterface to NonlinearSolvers\n\n\nThe \nevalResidual\n function and the fields \neqn.q\n and \neqn.res\n are the interface between the NonlinearSolvers and the physics modules.  The Nonlinear solvers populate \neqn.q\n, and use \nevalResidual\n to populate \neqn.res\n, from which the next value if \neqn.q\n is calculated.  The algorthmic differentiation mechanism described above is uses several residual evaluations to compute the Jacobian if needed for a given method.  Some methods, such as RK4, are better expressed in terms of \neqn.q_vec\n and \neqn.res_vec\n rather than \neqn.q\n and \neqn.res\n.  \neqn.assembleSolution\n and \neqn.disassmbleSolution\n exist to transform back and forth between the vector and 3D array forms.  In order to compute the Jacobian efficiently, it is necessary to work with the 3D arrays. For this reason, \nevalResidual\n must work only with \neqn.q\n and \neqn.res\n and let the caller decide whether or not to transform into the vector form.\n\n\nNewton's method supports both finite differences and complex step for calculating the Jacobian, and the static parameters need to be set accordingly. If finite differences are used, then \nTsol=Tres=Tmsh=Float64\n is required.  If complex step is used, then \nTsol=Tres=Complex128\n and \nTmsh = Float64\n is needed.\n\n\n\n\nInterface to users\n\n\nThe interface to users is described in \nsrc/Readme.md\n\n\n\n\nInterface to Summation-by-Parts\n\n\nThe physics modules use the interface provided by the Summation-by-Parts package to approximate derivatives numerically.  The reason for passing around the \nsbp\n object is that the SBP interface requires it to be passed in along with the data arrays it operates on.\n\n\n\n\nFunctional Programming\n\n\nAn important aspect of the use of the \nmesh\n, \nsbp\n, \neqn\n, and \nopts\n to define interfaces is that the physics modules and nonlinear solvers are written in a purely functional programming style, which is to say that the behavior of every function is determined entirely by the arguments to the function, and the only effects of the function are to modify an argument or return a value.\n\n\nThis property is important for writing generic, reusable code. For example, when using iterative solvers, a preconditioner is usually required, and constructing the preconditioner requires doing residual evaluations. In some cases, the preconditioner will use a different mesh or a different mesh coloring. Because the physics modules are written in a functional style, they can be used to evaluate the residual on a different mesh simply by passing the residual evaluation function a different mesh object.\n\n\nA critical aspect of function programming is that there is \nno global state\n. The state of the solver is determined entirely by the state of the objects that are passed around.\n\n\nVariable Naming Conventions In an attempt to make code more uniform and readable, certain variable names are reserved for certain uses.\n\n\n\n\nmesh\n:  object that implements \nAbstractMesh\n\n\npmesh\n:  mesh used for preconditioning\n\n\nsbp\n:  Summation-by-Parts object\n\n\neqn\n:  object that implements \nAbstractSolutionData\n\n\nopts\n: options dictionary\n\n\nparams\n: parameter object (used for values that might be in \nopts\n but need to be accessed quickly)\n\n\nx\n: the first real coordinate direction\n\n\ny\n: the second real coordinate direction\n\n\nxi\n: the first parametric coordinate direction\n\n\neta\n: the second parametric coordinate direction\n\n\nh\n:  mesh spacing\n\n\nhx\n: mesh spacing in x direction\n\n\nhy\n: mesh spacing in y direction\n\n\np\n: pressure at a node\n\n\na\n; speed of sound at a node\n\n\ns\n: entropy at a node\n\n\ngamma\n: specific heat ratio\n\n\ngamma_1\n: \ngamma\n - 1\n\n\nR\n: specific gas constant in ideal gas law (units J/(Kg * K) in SI)\n\n\ndelta_t\n: time step\n\n\nt\n: current time\n\n\nnrm\n: a normal vector of some kind\n\n\nA0\n: the coefficient matrix of the time term at a node\n\n\nAxi\n: the flux jacobian at a node in the \nxi\n direction\n\n\nAeta\n: the flux jacobian at a node in the \neta\n direction\n\n\nAx\n: the flux jacobian in the \nx\n direction at a node\n\n\nAy\n: the flux jacobian in the \ny\n direction at a node", 
            "title": "Code Interfaces"
        }, 
        {
            "location": "/interfaces/#interfaces-in-pdesolver", 
            "text": "PDESolver depends on the the three main objects, the  AbstractSolutionData  object,   AbstractMesh  object, and the SBP object implementing certain interfaces. This document describes what the interfaces are, and gives some hints for how to  implement them.  Before doing so, a short description of what general Julia interfaces look like is in order.   The paradigm of Julia code is that of \"objects with associated functions\", where  a new Type is defined, and then functions that take the Type as an argument are defined. The functions define the interface to the Type. The Type holds data (ie. state), and the functions perform operations on that state (ie. behavior). Perhaps counter-intuitively, it is generally not recommended for users of a type to access the fields directly. Instead, any needed operations on the data that the Type holds should be provided through functions. The benefit of this convention is that it imposes no requirements on how the Type stores its data or implements its behavior. This is important because a user of the Type should not be concerned with these things. The user needs to know what behavior the Type has, but not how it is implemented. This distinction becomes even more important when there are multiple implementations certain functionality. The user should be able to seamlessly transition between different implementations. This requires all implementations have the same interface.  The question of how to enforce interfaces, and how strongly to do so, is still an open question in Julia. Some relevant Github issues: *  5  *  4935  *  6975  One of the strongest arguments against the \"functions as interfaces\" idea is that for many applications, just storing data in an array is best. Creating interface functions for the Type to implement the array interface would be a lot of extra code with no benefit. For this reason, it makes sense to directly access the fields of some Types, to avoid trivial get/set methods. We do this extensively in PDESolver, because arrays are the natural choice for storing the kind of data used in PDESolver.", 
            "title": "Interfaces in PDESolver"
        }, 
        {
            "location": "/interfaces/#abstractsolutiondata", 
            "text": "ODLCommonTools defines:  abstract AbstractSolutionData{Tsol, Tres} .  The purpose of an  AbstractSolutionData  is to hold all the data related to the solution of an equation. This includes the solution at every node and any auxiliary quantities. The storage for any quantity that is calculated over the entire mesh should be allocated as part of this object, in order to avoid repeatedly reallocated the array for every residual evaluation. In general, there should never be a need to allocate a vector longer than the number of degrees of freedom at a node (or a matrix similarly sized matrix) during a residual evaluation. Structuring code such that it conforms with this requirement has significant performance benefits because it reduces memory allocation/deallocation.  The static parameter  Tsol  is the datatype of the solution variables and  Tres  is the datatype of the residual (when computing the Jacobian with finite differences or algorithmic differentiation, these will be the same).", 
            "title": "AbstractSolutionData"
        }, 
        {
            "location": "/interfaces/#required-fields", 
            "text": "The required fields of an  AbstractSolutionData  are:    q::AbstractArray{Tsol, 3}\n  q_vec::AbstractArray{Tsol, 1}\n  shared_data::AbstractArray{SharedFaceData{Tsol}, 1}\n  res::AbstractArray{Tres, 3}\n  res_vec::AbstractArray{Tres, 1}\n  M::AbstractArray{Float64, 1}\n  Minv::AbstractArray{Float64, 1}\n  disassembleSolution::Function\n  assembleSolution::Function\n  multiplyA0inv::Function\n  majorIterationCallback::Function\n  params{Tsol..., Tdim}::AbstractParamType{Tdim}  The purpose of these fields are:  q : to hold the solution variables in an element-based array.      This array should be  numDofPerNode  x  numNodesPerElement  x  numEl .      The residual evaluation  only  uses  q , never  q_vec  q_vec : to hold the solution variables as a vector, used for any linear algebra operations and time stepping. This array should have a length equal to the total number of degrees of freedom in the mesh. Even though this vector is not used by the residual evaluation, it is needed for many other operations, so it is allocated here so the memory can be reused. There are functions to facilitate the scattering of values from  q_vec  to  q . Note that for Continuous Galerkin type discretization (as opposed to Discontinuous Galerkin discretizations), there is not a corresponding \"gather\" operation (ie.  q  -   q_vec ).  shared_data  is a vector of length  npeers .  Each element contains the data               needed send and receive the  q  variables to/from other               the corresponding MPI rank listed in  mesh.peer_parts .               The precise contents of  SharedFaceData  is documented in the                Utils  module, however they do include the send and receive               buffers.  res : similar to  q , except that the residual evaluation function populates        it with the residual values.          As with  q , the residual evaluation function only interacts with this array,        never with  res_vec .  res_vec : similar to  q_vec .  Unlike  q_vec  there are functions to perform an            additive reduction (basically a \"gather\") of  res  to  res_vec .              For continuous Galerkin discretizations, the corresponding \"scatter\"            (ie.  res_vec  -  res`) may not exist.  M :  The mass matrix of the entire mesh.  Because SBP operators have diagonal       mass matrices, this is a vector.  Length numDofPerNode x numNodes (where       numNodes is the number of nodes in the entire mesh).  Minv :  The inverse of the mass matrix.  disassembleSolution :  Function that takes the a vector such as  q_vec  and                         scatters it to an array such as  q .                         This function must have the signature:                          disassembleSolution(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, q_arr:AbstractArray{T, 3}, q_vec::AbstractArray{T, 1})                          Because this variable is a field of a type, it will be dynamically dispatched.                         Although this is slower than compile-time dispatch, the cost is insignificant compared to the cost of evaluating the residual, so the added flexibility of having this function as a field is worth the cost.  assembleSolution :  Function that takes an array such as  res  and performs an additive reduction to a vector such as  res_vec .                      This function must have the signature:                       assembleSolution(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, res_arr::AbstractArray{T, 3}, res_vec::AbstractArray{T, 1}, zero_resvec=true)                       The argument  zero_resvec  determines whether  res_vec  is zeroed before the reduction is performed.                      Because it is an additive reduction, elements of the vector are only added to, never overwritten, so forgetting to zero out the vector could cause strange results.                      Thus the default is true.  multiplyA0inv :  Multiplies the solution values at each node in an array such as  res  by the inverse of the coefficient matrix of the time term of the equation.                   This function is used by time marching methods.                   For some equations, this matrix is the identity matrix, so it can be a no-op, while for others might not be.                   The function must have the signature:  multiplyA0inv(mesh::AbstractMesh, sbp, eqn::AbstractSolutionData, opts, res_arr::AbstractArray{Tsol, 3})  majorIterationCallback :  function called before every step of Newton's method or stage of an explicit time marching scheme. This function is used to do output and logging. The function must have the signature:  function majorIterationCallback(itr, mesh::AbstractMesh, sbp::AbstractSBP, eqn::AbstractEulerData, opts)  params :  user defined type that inherits from  AbstractParamType . The purpose of this type is to store any variables that need to be quickly accessed or updated. The only required fields are: *  t::Float64 : hold the current time value *  order : order of accuracy of the discretization (same as  AbstractMesh.order ) *   time::Timings : an object to record how long different parts of the code take,   defined in the Utils module.", 
            "title": "Required Fields"
        }, 
        {
            "location": "/interfaces/#abstractmesh", 
            "text": "ODLCommonTools defines:  abstract AbstractMesh{Tmsh} .  The purpose of an  AbstractMesh  is to hold all the mesh related data that the  solver will need.  It also serves to establish an interface between the solver  and whatever mesh software is used.  By storing all data in the fields of the   AbstractMesh  object, the details of how the mesh software stores and allows  retrieval of data are not needed by the solver.  This should make it easy to  accommodate different mesh software without making any changes to the solver.  The static parameter  Tmsh  is used to enable differentiation with respect to the mesh variable in the future.", 
            "title": "AbstractMesh"
        }, 
        {
            "location": "/interfaces/#required-fields_1", 
            "text": "# counts\n  numVert::Integer\n  numEl::Integer\n  numNodes::Integer\n  numDof::Integer\n  numDofPerNode::Integer\n  numNodesPerElement::Integer\n  order::Integer\n  numNodesPerFace::Int\n\n  # parallel counts\n  npeers::Int\n  numGlobalEl::Int\n  numSharedEl::Int\n  peer_face_counts::Array{Int, 1}\n  local_element_counts::Array{Int, 1}\n  remote_element_counts::array{Int, 1}\n\n  # MPI Info\n  comm::MPI.Comm\n  myrank::Int\n  commsize::Int\n  peer_parts::Array{Int, 1}\n\n  # Discretization type\n  isDG::Bool\n  isInterpolated::bool\n\n  # mesh data\n  coords::AbstractArray{Tmsh, 3}\n  dxidx::AbstractArray{Tmsh, 4}\n  jac::AbstractArray{Tmsh, 2}\n\n  # interpolated data\n  coords_bndry::Array{Tmsh, 3}\n  dxidx_bndry::Array{Tmsh, 4}\n  jac_bndry::Array{T1, 2}\n  dxidx_face::Array{Tmsh, 4}\n  jac_face::Array{Tmsh, 2}\n\n  # parallel data\n  coords_sharedface::Array{Array{Tmsh, 3}, 1}\n  dxidx_sharedface::Array{Array{Tmsh, 4}, 1}\n  jac_sharedface::Array{Array{Tmsh, 2}, 1}  \n\n  # boundary condition data\n  numBC::Integer\n  numBoundaryEdges::Integer\n  bndryfaces::AbstractArray{Boundary, 1}\n  bndry_offsets::AbstractArray{Integer, 1}\n  bndry_funcs::AbstractArray{BCType, 1}\n\n  # interior edge data\n  numInterfaces::Integer\n  interfaces::AbstractArray{Interface, 1}\n\n  # degree of freedom number data\n  dofs::AbstractArray{Integer, 2}\n  dof_offset::Int\n  sparsity_bnds::AbstractArray{Integer, 2}\n  sparsity_nodebnds::AbstractArray{Integer, 2}\n\n  # mesh coloring data\n  numColors::Integer\n  maxColors::Integer\n  color_masks::AbstractArray{ AbstractArray{Number, 1}, 1}\n  shared_element_colormasks::Array{Array{BitArray{1}, 1}, 1}\n  pertNeighborEls::AbstractArray{Integer, 2}\n\n  # parallel bookkeeping\n  bndries_local::Array{Array{Boundary, 1}, 1}\n  bndries_remote::Array{Array{Boundary, 1}, 1}\n  shared_interfaces::Array{Array{Interface, 1}, 1}\n  shared_element_offsets::Array{Int, 1}\n  local_element_lists::Array{Array{Int, 1}, 1}", 
            "title": "Required Fields"
        }, 
        {
            "location": "/interfaces/#counts", 
            "text": "numVert :  number of vertices in the mesh  numEl :  number of elements in the mesh  numNodes : number of nodes in the mesh  numDof :  number of degrees of freedom in the mesh (=  numNodes  *  numDofPerNode )  numDofPerNode :  number of degrees of freedom on each node.  numNodesPerElement :  number of nodes on each element.  order :  order of the discretization (ie. first order, second order...), where           an order  p  discretization should have a convergence rate of  p+1 .  numNodesPerFace : number of nodes on an edge in 2D or face in 3D.  For interpolated                    meshes it is the number of interpolation points on the face", 
            "title": "Counts"
        }, 
        {
            "location": "/interfaces/#parallel-counts", 
            "text": "npeers : number of processes that have elements that share a face with the current          process  numGlobalEl : number of locally owned elements + number of non-local elements that                share a face with a locally owned element  numSharedEl : number of non-local elements that share a face with a locally owned                element  local_element_counts : array of length  npeers , number of local elements                         that share a face with each remote process  remote_element_counts : array of length  npeers , number of remote elements                          that share a face with with current process", 
            "title": "Parallel Counts"
        }, 
        {
            "location": "/interfaces/#mpi-info", 
            "text": "comm : the MPI Communicator the mesh is defined on  myrank : rank of current MPI process (0-based)  commsize : number of MPI processes on this communicator  peer_parts : array of MPI proccess ranks for each process that has elements that               share a face with the current process", 
            "title": "MPI Info"
        }, 
        {
            "location": "/interfaces/#discretization-type", 
            "text": "isDG : true if mesh is a DG type discretization  isInterpolated : true if mesh requires data to be interpolated to the faces                   of an element", 
            "title": "Discretization Type"
        }, 
        {
            "location": "/interfaces/#mesh-data", 
            "text": "coords :  n  x  numNodesPerElement  x  numEl  array, where  n  is the dimensionality of            the equation being solved (2D or 3D typically).               coords[:, nodenum, elnum] = [x, y, z]  coordinates of node  nodenum             of element  elnum .  dxidx :   n  x  n  x  numNodesPerElement  x  numEl , where  n  is defined above. It stores the mapping jacobian scaled by  ( 1/det(jac) dxi/dx )  where  xi  are the parametric coordinates,  x  are the physical (x,y,z) coordinates, and  jac  is the determinant of the mapping jacobian  dxi/ dx .  jac   :  numNodesPerElement  x  numEl  array, holding the determinant of the          mapping jacobian  dxi/dx  at each node of each element.", 
            "title": "Mesh Data"
        }, 
        {
            "location": "/interfaces/#interpolated-data", 
            "text": "This data is used for interpolated mesh only.  coords_bndry : coordinates of nodes on the boundary of the mesh,                 2 x  numFaceNodes  x  numBoundaryEdges  dxidx_bndry : 2 x 2 x  numFaceNodes  x  numBoundary edges array of dxidx`                interpolated to the boundary of the mesh  jac_bndry :  numFaceNodes  x  numBoundaryEdges  array of  jac  interpolated               to the boundary  dxidx_face : 2 x 2 x  numFaceNodes  x  numInterfaces  array of  dxidx                interpolated to the face shared between two elements  jac_face :  numNodesPerFace  x  numInterfaces  array of  jac  interpolated              to the face shared between two element", 
            "title": "Interpolated Data"
        }, 
        {
            "location": "/interfaces/#parallel-data", 
            "text": "This data is required for parallelizing interpolated DG meshes  coords_sharedface : array of arrays, one array for each peer process,                      containing the coordinates of the nodes on the faces                      shared between a local element on a non-local element.                      Each array is 2 x  numFaceNodes  x number of faces shared                      with this process.  dxidx_sharedface : similar to  coords_sharedface ,  dxidx  interpolated to                     faces between elements in different processes, each                     array is 2 x 2 x  numFaceNodes  x number of faces shared                     with this process.  jac_sharedface : similar to  coords_sharedface ,  jac  interpolated to faces                   between a local element and a non-local element. Each array                   is  numFaceNodes  x number of faces shared with this process.", 
            "title": "Parallel Data"
        }, 
        {
            "location": "/interfaces/#boundary-condition-data", 
            "text": "The mesh object stores data related to applying boundary conditions. Boundary conditions are imposed weakly, so there is no need to remove degrees of freedom from the mesh when Dirichlet boundary conditions are applied. In order to accommodate any combination of boundary conditions, an array of functors are stored as part of the mesh object, along with lists of which mesh edges (or faces in 3D) should have which boundary condition applied to them  numBC : number of different types of boundary conditions used.  numBoundaryEdges : number of mesh edges that have boundary conditions applied                     to them.  bndryfaces :  array of Boundary objects (which contain the element number and                the local index of the edge), of length  numBoundaryEdges .  bndry_offsets :  array of length numBC+1, where  bndry_offsets[i]  is the index                   in  bndryfaces  where the edges that have boundary condition                    i  applied to them start.                   The final entry in  bndry_offsets  should be  numBoundaryEdges + 1 .                   Thus  bndryfaces[ bndry_offsets[i]:(bndry_offsets[i+1] - 1) ]                    contains all the boundary edges that have boundary condition                    i  applied to them.  bndry_funcs :  array of boundary functors, length  numBC .  All boundary                 functors are subtypes of  BCType .  Because  BCType  is an                 abstract type, the elements of this array should not be used                 directly, but passed as an argument to another function, to                  avoid type instability.", 
            "title": "Boundary Condition Data"
        }, 
        {
            "location": "/interfaces/#interior-edge-data", 
            "text": "Data about interior mesh edges (or faces in 3D) is stored to enable use of edge stabilization or Discontinuous Galerkin type discretizations. Only data for edges (faces) that are shared by two elements are stored (ie. boundary edges are not considered).  numInterfaces :  number of interior edges  interfaces :  array of Interface types (which contain the element numbers for                the two elements sharing the edge, and the local index of the                edge from the perspective of the two elements, and an indication                of the relative edge orientation).                The two element are referred to as  elementL  and  elementR ,                but the choice of which element is  elementL  and which is                 elementR  is arbitrary.                The length of the array is numInterfaces.               Unlike  bndryfaces , the entries in the array do not have to be in               any particular order.", 
            "title": "Interior Edge Data"
        }, 
        {
            "location": "/interfaces/#degree-of-freedom-numbering-data", 
            "text": "dofs :   numDofPerNode  x  numNodesPerElement  x  numEl  array. Holds the local degree of freedom number of each degree of freedom. Although the exact method used to assign dof numbers is not critical, all degrees of freedom on a node must be numbered sequentially.  dof_offset : offset added to the local dof number to make it a global dof               number.  sparsity_bnds :  2 x  numDof  array.  sparsity_bnds[:, i]  holds the maximum, minimum degree of freedom numbers associated with degree of freedom  i . In this context, degrees of freedom  i  and  j  are associated if entry  (i,j)  of the jacobian is non-zero. In actuality,  sparsity_bnds  need only define upper and lower bounds for degree of freedom associations (ie. they need not be tight bounds). This array is used to to define the sparsity pattern of the jacobian matrix.  sparsity_nodebnds :  2 x numNodes array.  sparsity_bnds[:, i]  holds the maximum, minimum node associated with node  i , similar the information stored in  sparsity_bnds  for degrees of freedom.", 
            "title": "Degree of Freedom Numbering Data"
        }, 
        {
            "location": "/interfaces/#mesh-coloring-data", 
            "text": "The NonlinearSolvers module uses algorithmic differentiation to compute the Jacobian. Doing so efficiently requires perturbing multiple degrees of freedom simultaneously, but perturbing associated degrees of freedom at the same time leads to incorrect results. Mesh coloring assigns each element of the mesh to a group (color) such that every degree of freedom on each element is not associated with any other degree  of freedom on any other element of the same color. An important aspect of satisfying this condition is the use of the  element-based arrays (all arrays that store data for a quantity over the entire  mesh are  ncomp  x  numNodesPerElement  x  numEl ). In such an array, any node that is part of 2 or more elements has one entry for each element. When performing algorithmic differentiation, this enables perturbing a degree of  freedom on one element without perturbing it on the other elements that share  the degree of freedom.  For example, consider a node that is shared by two elements. Let us say it is node 2 of element 1 and node 3 of element 2. This means  AbstractSolutionData.q[:, 2, 1]  stores the solution variables for this node on the first element, and  AbstractSolutionData.q[:, 3, 2]  stores the solution variables for the second element. Because these are different entries in the array  AbstractSolutionData.q , they can be perturbed independently. Because  AbstractSolutionData.res  has the same format, the perturbations to  AbstractSolutionData.q[:, 2, 1]  are mapped to  AbstractSolutionData.res[:, 2, 1]  for a typical continuous Galerkin type discretization. This is a direct result of having an element-based discretization.  There are some discretizations, however, that are not strictly element-based. Edge stabilization, for example, causes all the degrees of freedom of one element to be associated with any elements it shares an edge with. To deal with this, we use the idea of a distance-n coloring. A distance-n coloring is a coloring where there are n elements in between two element of the same color. For element-based discretizations with element-based arrays, every element in the mesh can be the same color.  This is a distance-0 coloring. For an edge stabilization discretization, a distance-1 coloring is required, where every element is a different color than any neighbors it shares and edge with. (As a side node, the algorithms that perform a distance-1 coloring are rather complicated, so in practice we use a distance-2 coloring instead).  In order to do algorithmic differentiation, the  AbstractMesh  object must store the information that determines which elements are perturbed for which colors, and, for the edge stabilization case, how to relate a perturbation in the output   AbstractSolutionData.res  to the degree of freedom in  AbstractSolutionData.q  in O(1) time. Each degree of freedom on an element is perturbed independently of the other degrees of freedom on the element, so the total number of residual evaluations is the number of colors times the number of degrees of freedom on an element.  The fields required are:  numColors :  The number of colors in the mesh.  maxColors : the maximum number of colors on any process  color_masks :  array of length  numColors .  Each entry in the array is itself                 an array of length  numEl .  Each entry of the inner array is                 either a 1 or a 0, indicating if the current element is                 perturbed or not for the current color. For example, in  color_mask_i = color_masks[i]; mask_elj = color_mask_i[j] , the variable  mask_elj  is either a 1 or a zero, determining whether or not element  j  is perturbed as part of color  i .  shared_element_colormasks : array of BitArrays controlling when to perturb                              non-local elements.  There are  npeers  arrays,                              each of length number of non-local elements shared                              with this process  pertNeighborEls :   numEl  x  numColors  array.   neighbor_nums[i,j]  is the element number of of the element whose perturbation is affected element  i  when color  j  is being perturbed, or zero if element  i  is not affected by any  perturbation.", 
            "title": "Mesh Coloring Data"
        }, 
        {
            "location": "/interfaces/#parallel-bookkeeping", 
            "text": "bndries_local : array of arrays of  Boundary s describing faces shared                  with non-local elements from the local side (ie. the                  local element number and face).  The number of arrays is                   npeers .  bndries_remote : similar to  bndries_local , except describing the faces                   from the non-local side.  Note that the element numbers are                   from the  remote  process  shared_interfaces : array of arrays of  Interface s describing the faces                      shared between local and non-local elements.  The local                      element is  always   elementL .  The remote element is                      assigned a local number greater than  numEl .  shared_element_offsets : array of length npeers+1 that contains the first                           element number assigned to elements on the shared                           interface.  The last entry is equal to the highest                           element number on the last peer process + 1.  The                           elements numbers assigned to a given peer must form                           a contiguous range.  local_element_lists : array of arrays containing the element numbers of the                        elements that share a face with each peer process", 
            "title": "Parallel Bookkeeping"
        }, 
        {
            "location": "/interfaces/#other-functions", 
            "text": "The mesh module must also defines and exports the functions  saveSolutionToMesh(mesh::MeshImplementationType, vec::AbstractVector)\nwriteVisFiles(mesh::MeshImplementationType, fname::ASCIIString)  where the first function takes a vector of length  numDof  and saves it to the mesh, and the second writes Paraview files for the mesh, including the solution field.", 
            "title": "Other Functions"
        }, 
        {
            "location": "/interfaces/#physics-module", 
            "text": "For every new physics to be solved, a new module should be created. The purpose of this module is to evaluate the equation:  M dq/dt = f(q)  where  M  is the mass matrix. For steady problems,  dq/dt = 0  and the module evaluates the residual. For unsteady problems, the form  M dq/dt = f(q)  is suitable for explicit time marching.  Every physics module must define a boundary condition called  defaultBC .  If the user does not specify a boundary condition on any geometric edges, the mesh constructor will add a new boundary condition and assign all mesh edges classified on the unspecified geometric edges are assigned to it.  This boundary condition can be a no-op if that is correct for the physics.", 
            "title": "Physics Module"
        }, 
        {
            "location": "/interfaces/#interface-to-nonlinearsolvers", 
            "text": "The  evalResidual  function and the fields  eqn.q  and  eqn.res  are the interface between the NonlinearSolvers and the physics modules.  The Nonlinear solvers populate  eqn.q , and use  evalResidual  to populate  eqn.res , from which the next value if  eqn.q  is calculated.  The algorthmic differentiation mechanism described above is uses several residual evaluations to compute the Jacobian if needed for a given method.  Some methods, such as RK4, are better expressed in terms of  eqn.q_vec  and  eqn.res_vec  rather than  eqn.q  and  eqn.res .   eqn.assembleSolution  and  eqn.disassmbleSolution  exist to transform back and forth between the vector and 3D array forms.  In order to compute the Jacobian efficiently, it is necessary to work with the 3D arrays. For this reason,  evalResidual  must work only with  eqn.q  and  eqn.res  and let the caller decide whether or not to transform into the vector form.  Newton's method supports both finite differences and complex step for calculating the Jacobian, and the static parameters need to be set accordingly. If finite differences are used, then  Tsol=Tres=Tmsh=Float64  is required.  If complex step is used, then  Tsol=Tres=Complex128  and  Tmsh = Float64  is needed.", 
            "title": "Interface to NonlinearSolvers"
        }, 
        {
            "location": "/interfaces/#interface-to-users", 
            "text": "The interface to users is described in  src/Readme.md", 
            "title": "Interface to users"
        }, 
        {
            "location": "/interfaces/#interface-to-summation-by-parts", 
            "text": "The physics modules use the interface provided by the Summation-by-Parts package to approximate derivatives numerically.  The reason for passing around the  sbp  object is that the SBP interface requires it to be passed in along with the data arrays it operates on.", 
            "title": "Interface to Summation-by-Parts"
        }, 
        {
            "location": "/interfaces/#functional-programming", 
            "text": "An important aspect of the use of the  mesh ,  sbp ,  eqn , and  opts  to define interfaces is that the physics modules and nonlinear solvers are written in a purely functional programming style, which is to say that the behavior of every function is determined entirely by the arguments to the function, and the only effects of the function are to modify an argument or return a value.  This property is important for writing generic, reusable code. For example, when using iterative solvers, a preconditioner is usually required, and constructing the preconditioner requires doing residual evaluations. In some cases, the preconditioner will use a different mesh or a different mesh coloring. Because the physics modules are written in a functional style, they can be used to evaluate the residual on a different mesh simply by passing the residual evaluation function a different mesh object.  A critical aspect of function programming is that there is  no global state . The state of the solver is determined entirely by the state of the objects that are passed around.", 
            "title": "Functional Programming"
        }, 
        {
            "location": "/interfaces/#variable-naming-conventions-in-an-attempt-to-make-code-more-uniform-and-readable-certain-variable-names-are-reserved-for-certain-uses", 
            "text": "mesh :  object that implements  AbstractMesh  pmesh :  mesh used for preconditioning  sbp :  Summation-by-Parts object  eqn :  object that implements  AbstractSolutionData  opts : options dictionary  params : parameter object (used for values that might be in  opts  but need to be accessed quickly)  x : the first real coordinate direction  y : the second real coordinate direction  xi : the first parametric coordinate direction  eta : the second parametric coordinate direction  h :  mesh spacing  hx : mesh spacing in x direction  hy : mesh spacing in y direction  p : pressure at a node  a ; speed of sound at a node  s : entropy at a node  gamma : specific heat ratio  gamma_1 :  gamma  - 1  R : specific gas constant in ideal gas law (units J/(Kg * K) in SI)  delta_t : time step  t : current time  nrm : a normal vector of some kind  A0 : the coefficient matrix of the time term at a node  Axi : the flux jacobian at a node in the  xi  direction  Aeta : the flux jacobian at a node in the  eta  direction  Ax : the flux jacobian in the  x  direction at a node  Ay : the flux jacobian in the  y  direction at a node", 
            "title": "Variable Naming Conventions In an attempt to make code more uniform and readable, certain variable names are reserved for certain uses."
        }, 
        {
            "location": "/parallel/", 
            "text": "Parallel Overview\n\n\nThis document describes how PDEs are solved in parallel.\n\n\nIn general, the mesh is partitioned and each part is assigned to a different MPI process. Each element is owned by exactly one process.  During  initialization, the mesh constructor on each process figures out which other processes have elements that share a face (edge in 2D) with local elements. It counts how many faces and elements are shared (a single element could have multiple faces on the parallel boundary), and assigns local number to both the elements and the degrees of freedom on the elements.  The non-local elements  are given numbers greater than \nnumEl\n, the number of local elements.   The degrees of freedom are re-numbered such that newly assigned dof number plus the \ndof_offset\n for the current process equals the global dof number, which is defined by the local dof number assigned by the process that owns the element.  As a  result, dof numbers for elements that live on processes with lower ranks will be negative.\n\n\nAs part of counting the number of shared faces and elements, 3 arrays are formed: \nbndries_local\n, \nbndries_remote\n, and \nshared_interfaces\n which  describe the shared faces from the local side, the remote side, and a  unified view of the interface, respectively.  This allows treating the shared faces like either a special kind of boundary condition or a proper  interface, similar to the interior interfaces.\n\n\nThere are 2 modes of parallel operation, one for explicit time marching and the other for Newton's method.\n\n\n\n\nExplicate Time Marching\n\n\nIn this mode, each process each process sends the solution values at the  shared faces to the other processes.  Each process then evaluates the residual using the received values and updates the solution.\n\n\nThe function \nexchangeFaceData\n is designed to perform the sending and  receiving of data.  Non-blocking communications are used, and the function does not wait for the communication to finish before returning.  The  MPI_Requests for the sends and receives are stored in the appropriate fields of the mesh.  It is the responsibility of each physics module call  \nexchangeFaceData\n and to wait for the communication to finish before using the data.  Because the receives could be completed in any order, it is  recommended to use \nMPI_Waitany\n to wait for the first receive to complete,  do as many computations as possible on the data, and then call \nMPI_Waitany\n again for the next receive.\n\n\n\n\nNewtons Method\n\n\nFor Newton's method, each process sends the solution values for all the  elements on the shared interface at the beginning of a Jacobian calculation.  Each process is then responsible for perturbing the solutions values of both  the local and non-local elements.  The benefit of this is that parallel  communication is required once per Jacobian calculation, rather than once  per residual evaluation as with the explicit time marching mode.\n\n\nThe function \nexchangeElementData\n copies the data from the shared elements into the send buffer and sends it, and also posts the corresponding receives. It does not wait for the communications to finish before returning.   The function is called by Newton's method after a new solution is calculated, so the physics module does not have to do it, but the physics module does  have to wait for the receives to finish before using the data.  This is  necessary to allow overlap of the communication with computation.  As  with the explicit time marching mode, use of \nMPI_Waitany\n is recommended.", 
            "title": "Code Parallelization"
        }, 
        {
            "location": "/parallel/#parallel-overview", 
            "text": "This document describes how PDEs are solved in parallel.  In general, the mesh is partitioned and each part is assigned to a different MPI process. Each element is owned by exactly one process.  During  initialization, the mesh constructor on each process figures out which other processes have elements that share a face (edge in 2D) with local elements. It counts how many faces and elements are shared (a single element could have multiple faces on the parallel boundary), and assigns local number to both the elements and the degrees of freedom on the elements.  The non-local elements  are given numbers greater than  numEl , the number of local elements.   The degrees of freedom are re-numbered such that newly assigned dof number plus the  dof_offset  for the current process equals the global dof number, which is defined by the local dof number assigned by the process that owns the element.  As a  result, dof numbers for elements that live on processes with lower ranks will be negative.  As part of counting the number of shared faces and elements, 3 arrays are formed:  bndries_local ,  bndries_remote , and  shared_interfaces  which  describe the shared faces from the local side, the remote side, and a  unified view of the interface, respectively.  This allows treating the shared faces like either a special kind of boundary condition or a proper  interface, similar to the interior interfaces.  There are 2 modes of parallel operation, one for explicit time marching and the other for Newton's method.", 
            "title": "Parallel Overview"
        }, 
        {
            "location": "/parallel/#explicate-time-marching", 
            "text": "In this mode, each process each process sends the solution values at the  shared faces to the other processes.  Each process then evaluates the residual using the received values and updates the solution.  The function  exchangeFaceData  is designed to perform the sending and  receiving of data.  Non-blocking communications are used, and the function does not wait for the communication to finish before returning.  The  MPI_Requests for the sends and receives are stored in the appropriate fields of the mesh.  It is the responsibility of each physics module call   exchangeFaceData  and to wait for the communication to finish before using the data.  Because the receives could be completed in any order, it is  recommended to use  MPI_Waitany  to wait for the first receive to complete,  do as many computations as possible on the data, and then call  MPI_Waitany  again for the next receive.", 
            "title": "Explicate Time Marching"
        }, 
        {
            "location": "/parallel/#newtons-method", 
            "text": "For Newton's method, each process sends the solution values for all the  elements on the shared interface at the beginning of a Jacobian calculation.  Each process is then responsible for perturbing the solutions values of both  the local and non-local elements.  The benefit of this is that parallel  communication is required once per Jacobian calculation, rather than once  per residual evaluation as with the explicit time marching mode.  The function  exchangeElementData  copies the data from the shared elements into the send buffer and sends it, and also posts the corresponding receives. It does not wait for the communications to finish before returning.   The function is called by Newton's method after a new solution is calculated, so the physics module does not have to do it, but the physics module does  have to wait for the receives to finish before using the data.  This is  necessary to allow overlap of the communication with computation.  As  with the explicit time marching mode, use of  MPI_Waitany  is recommended.", 
            "title": "Newtons Method"
        }, 
        {
            "location": "/pdesolver/", 
            "text": "Documentation of the PDESolver Module\n\n\n\n\nDocumentation of the PDESolver User Facing Interface\n\n\nDocumentation of the PDESolver Physics Module Interface", 
            "title": "Introduction"
        }, 
        {
            "location": "/pdesolver/#documentation-of-the-pdesolver-module", 
            "text": "Documentation of the PDESolver User Facing Interface  Documentation of the PDESolver Physics Module Interface", 
            "title": "Documentation of the PDESolver Module"
        }, 
        {
            "location": "/pdesolver_user/", 
            "text": "Documentation of the PDESolver User Facing Interface", 
            "title": "PDESolver User Interface"
        }, 
        {
            "location": "/pdesolver_user/#documentation-of-the-pdesolver-user-facing-interface", 
            "text": "", 
            "title": "Documentation of the PDESolver User Facing Interface"
        }, 
        {
            "location": "/pdesolver_physics/", 
            "text": "Documentation of the PDESolver Physics Module Interface", 
            "title": "PDESolver Physics Interface"
        }, 
        {
            "location": "/pdesolver_physics/#documentation-of-the-pdesolver-physics-module-interface", 
            "text": "", 
            "title": "Documentation of the PDESolver Physics Module Interface"
        }, 
        {
            "location": "/invocation/calling/", 
            "text": "", 
            "title": "Calling Ticon"
        }, 
        {
            "location": "/invocation/interactive/", 
            "text": "Running Ticon in an interactive session using Julia's REPL\n\n\nJulia's REPL is a powerful tool for debugging.  Being able to run commands interactively can reveal important information   that would otherwise be available only through cumbersome print statements and  error logs.\n\n\nThis page describes the first steps involved for running Ticon interactively. Calling Ticon from Julia's REPL in the fashion described here is an experimental    method, and is not tested consistently. Additionally, not all parts of Ticon are usable after the steps below. Instead, this document is intended to provide a springing-off point for the developer    who wishes to run Ticon interactively, and can adapt the commands below to their needs.\n\n\n\n\nScript\n\n\nScript 123", 
            "title": "Interactive Session (experimental)"
        }, 
        {
            "location": "/invocation/interactive/#running-ticon-in-an-interactive-session-using-julias-repl", 
            "text": "Julia's REPL is a powerful tool for debugging.  Being able to run commands interactively can reveal important information   that would otherwise be available only through cumbersome print statements and  error logs.  This page describes the first steps involved for running Ticon interactively. Calling Ticon from Julia's REPL in the fashion described here is an experimental    method, and is not tested consistently. Additionally, not all parts of Ticon are usable after the steps below. Instead, this document is intended to provide a springing-off point for the developer    who wishes to run Ticon interactively, and can adapt the commands below to their needs.", 
            "title": "Running Ticon in an interactive session using Julia's REPL"
        }, 
        {
            "location": "/invocation/interactive/#script", 
            "text": "Script 123", 
            "title": "Script"
        }, 
        {
            "location": "/solver/Readme/", 
            "text": "Overview of Physics Modules\n\n\n\n\nAbstractSolutionData\n and Physics Module Implementation\n\n\nThis document describes some best practices for implementing a physics module. These practices are not required, but have proven to be useful for producing organized, readable, and reusable code.\n\n\n\n\nLevels of Functions\n\n\nIt is useful to divide functions into 3 catagories, high, mid, and low level functions.  The purpose of high level functions is to decide which method of performing an operation should be used and call other functions to do it. For example, the Euler physics modules has \nevalVolumeIntegrals\n and \nevalBoundaryIntegrals\n as high level functions.  There are several different ways of calculating both the volume and boundary integrals.  The options dictionary is used to decide what mid level function to call.  Each mid level function implements a different way of doing the calculation.\n\n\nThe purpose of mid level functions is to loop over the mesh and call a low level function for each node.  For example, the function \ngetEulerFlux\n loops over the nodes of the mesh and calls a function to calculate the Euler flux at each node.  Mid level function names usually start with \nget\n to indicate that their purpose is to calculate some quantity but don't do the calculation themselves.\n\n\nLow level functions calculate a quantity at a node.  For example, \ncalcEulerFlux\n calculates the Euler flux at a single node.  Low level function names usually start with \ncalc\n to indicate that they perform a specific calculation. Often, different discretizations use the same structure of loops, but do a slightly different calculation at each node.  Low level functions are called inside the innermost loop of the code, so it would be too expensive to have if statements to select which low level function to call, so various tricks involving Julia's multiple dispatch system are used to get the compiler to decide which low level function to call.  These will be described later in this document.\n\n\nIt is often useful to dispatch to low level functions based on \nTdim\n and \nvar_type\n.  For this reason the Euler equation implementation of \nAbstractParamType\n is\n\n\ntype ParamType{Tdim, var_type, Tsol, Tres, Tmsh} \n: AbstractParamType{Tdim}\n\n\n\n\nThe other static parameters are necessary because \nParamType\n has fields of those datatypes.\n\n\n\n\nAbstractSolutionData\n implementation\n\n\nEach physics module should define and export a subtype of \nAbstractSolutionData{Tsol, Tres}\n. The implementation of \nAbstractSolutionData{Tsol, Tres}\n must inherit the \nTsol\n and \nTres\n static parameters, and may have additional static parameters as well. It may also be helpful to define additional abstract types within the physics module to provide different levels of abstractions. For example, the Euler physics module defines:\n\n\nabstract AbstractEulerData{Tsol, Tres} \n: AbstractSolutionData{Tsol, Tres}\nabstract EulerData {Tsol, Tdim, Tres, var_type} \n: AbstractEulerData{Tsol, Tres}\ntype EulerData_{Tsol, Tres, Tdim, Tmsh, var_type} \n: EulerData{Tsol, Tdim, Tres, var_type}\n\n\n\n\nThe first line is effectively just a name change and may not be necessary. The second line adds the static parameters \nTdim\n, and \nvar_type\n while inheriting the \nTsol\n and \nTres\n types from \nAbstractEulerData\n. \nTdim\n is the dimensionality of the equation, \nTres\n is the datatype of the residual variables, and \nvar_type\n is a symbol indicating whether the equation is being solved with conservative or entropy variables. The third line defines a concrete type that implements all the features required of an \nAbstractSolutionData\n, and adds a static parameter \nTmsh\n, the datatype of the mesh variables.   The additional static parameter is necessary because one field of \nEulerData_\n has type \nTmsh\n. Note that there could be multiple implementations of \nAbstractSolutionData\n for the Euler equations, perhaps with different fields to store certain data or not. All these implementations will need to have the static parameters \nTsol\n, \nTdim\n, \nTres\n, and \nvar_type\n, so \nEulerData\n is defined as an abstract type,  allowing all implementations to inherit from it. All high level functions involved in evaluating the residual will take in an argument of type \nEulerData\n. Only when low level functions need to dispatch based on which implementation is  used would it take in an \nEulerData_\n or another implementation.\n\n\n\n\nVariable Conversion\n\n\nSome equations can be written in different variables, and need to convert between them.  To do this, it is \nfunction convertFromNaturalToWorkingVars{Tsol}(params::ParamType{2, :var_type},                qc::AbstractArray{Tsol,1}, qe::AbstractArray{Tsol,1})\n\n\nthat converts from the \"natural\" variables in which to write an equation to some other set of variables at a node.  For the Euler equations, the \"natural\" variables would be the conservative variables, and one example of \"other\" variables would be the entropy variables.\n\n\nIt is also sometimes useful to define the opposite conversion, ie. from the working variables to the natural variables.\n\n\n\n\nInput Options\n\n\nMany of the components of PDESolver have different options that control how they work and what they do. In order to  provide a unified method of specifying these options, an dictionary  of type \nDict{ASCIIString, Any}\n is read in from a disk file. This dictionary (called \nopts\n in function signatures), is passed to all high and mid level functions so they can use values in the dictionary to determine their  control flow. Low level functions need to be extremely efficient, so they cannot have conditional logic, therefore they are not passed the dictionary. Note that retrieving values from a dictionary is very slow compared to accessing the fields of a type, so all values that are accessed repeatedly should be stored  as the field of a type.\n\n\n\n\nFunctors\n\n\nFunctors are a trick used to get Julia's dispatch system to make decisions at compile time rather than runtime.  This is particularly useful for boundary conditions, where the list of mesh faces that have boundary conditions applied is determined at runtime, but having conditional statements that execute for every node on the mesh boundary would be slow.  Instead a construct is used as follows:\n\n\ntype myBC \n: BCType  # create a singleton type\nend\n\nfunction call(obj::myBC, q::AbstractVector, bndryflux::AbstractVector)\n  # calculate boundary flux here\nend\n\n\n\n\nThis defines a datatype and adds a method to the \ncall\n function for that type. The call function is what makes a datatype callable like a function.  This method is called as follows:\n\n\nfunctor = myBC()  # construct and object of type myBC\nq = rand(4)\nbndryflux = zeros(4)\nfunctor(q, bndryflux)  # the Julia compiler turns this into call(functor, q, bndryflux)  \n\n\n\n\nThe way this is used for boundary conditions is through a two level construct where an outer function passes a functor to an inner function.  Julia's JIT will generate a method of the inner function that is specialized to the functor (this is why it is important that the functor is a datatype).  For example:\n\n\nfunction getBCFluxes(mesh, sbp, eqn, opts)\n\n  for i=1:mesh.numBC  # loop over different boundary conditions\n    functor_i = mesh.bndry_functor[i]  # get the functor for this boundary condition\n    start_index = mesh.bndry_offsets[i]\n    end_index = mesh.bndry_offsets[i+1] - 1\n    # get data for boundary faces start_index:end_index\n\n    calcBoundaryFlux(functor_i, data for boundary faces start_index:end_index)\n  end\nend  # end function\n\n  function calcBoundaryFlux(functor_i::BCType, data for boundary faces start_index:end_index)\n    for i=1:length(start_index:end_index)\n      for j=1:num_nodes_on_face\n        # get data for this boundary face node\n        functor_i(data for this boundary face node)\n      end\n    end\n\n  end  # end function\n\n\n\n\nThe benefit of this arrangement is that \nmesh.numBC\n different version of calcBoundaryFlux get compiled, one for each functor, and each version knows about the \ncall\n method that was defined for the functor it is passed.  This two level scheme allows the compiler to make all the decisions about what function to call (ie. the \ncall\n method of the functor), avoiding any conditional logic at runtime\n\n\nThis idea is also applicable to the flux functions used by DG methods.\n\n\nInitialization of a Simulation\n\n\nThis section lists an outline of how a simulation gets launched After step 4, the procedure becomes a bit more complicated because there are optional steps. Only the required steps are listed below.\n\n\n\n\n\n\nThe options dictionary is read in.  Default values are supplied for any key that is not specified, if a reasonable default value exists.\n\n\n\n\n\n\nSecond, the \nsbp\n operator is constructed.\n\n\n\n\n\n\nThe \nmesh\n object is constructed, using the options dictionary and the \nsbp\n operator.  Some of the options in the dictionary are used to determine how the mesh gets constructed.  For example, the options dictionary specifies what kind of mesh coloring to do.\n\n\n\n\n\n\nThe \neqn\n object is constructed, using the \nmesh\n, \nsbp\n, and \nopts\n objects.\n\n\n\n\n\n\nThe physics module \ninit\n function is called, which initializes the physics module and finishes any initialization that \nmesh\n and \neqn\n objects require.\n\n\n\n\n\n\nThe initial condition is applied to \neqn.q_vec\n.\n\n\n\n\n\n\nA nonlinear solver is called.  Which solver is called and what parameters it uses are determined by the options dictionary.\n\n\n\n\n\n\nPost-processing is done, if required by the options dictionary.", 
            "title": "Introduction"
        }, 
        {
            "location": "/solver/Readme/#overview-of-physics-modules", 
            "text": "", 
            "title": "Overview of Physics Modules"
        }, 
        {
            "location": "/solver/Readme/#abstractsolutiondata-and-physics-module-implementation", 
            "text": "This document describes some best practices for implementing a physics module. These practices are not required, but have proven to be useful for producing organized, readable, and reusable code.", 
            "title": "AbstractSolutionData and Physics Module Implementation"
        }, 
        {
            "location": "/solver/Readme/#levels-of-functions", 
            "text": "It is useful to divide functions into 3 catagories, high, mid, and low level functions.  The purpose of high level functions is to decide which method of performing an operation should be used and call other functions to do it. For example, the Euler physics modules has  evalVolumeIntegrals  and  evalBoundaryIntegrals  as high level functions.  There are several different ways of calculating both the volume and boundary integrals.  The options dictionary is used to decide what mid level function to call.  Each mid level function implements a different way of doing the calculation.  The purpose of mid level functions is to loop over the mesh and call a low level function for each node.  For example, the function  getEulerFlux  loops over the nodes of the mesh and calls a function to calculate the Euler flux at each node.  Mid level function names usually start with  get  to indicate that their purpose is to calculate some quantity but don't do the calculation themselves.  Low level functions calculate a quantity at a node.  For example,  calcEulerFlux  calculates the Euler flux at a single node.  Low level function names usually start with  calc  to indicate that they perform a specific calculation. Often, different discretizations use the same structure of loops, but do a slightly different calculation at each node.  Low level functions are called inside the innermost loop of the code, so it would be too expensive to have if statements to select which low level function to call, so various tricks involving Julia's multiple dispatch system are used to get the compiler to decide which low level function to call.  These will be described later in this document.  It is often useful to dispatch to low level functions based on  Tdim  and  var_type .  For this reason the Euler equation implementation of  AbstractParamType  is  type ParamType{Tdim, var_type, Tsol, Tres, Tmsh}  : AbstractParamType{Tdim}  The other static parameters are necessary because  ParamType  has fields of those datatypes.", 
            "title": "Levels of Functions"
        }, 
        {
            "location": "/solver/Readme/#abstractsolutiondata-implementation", 
            "text": "Each physics module should define and export a subtype of  AbstractSolutionData{Tsol, Tres} . The implementation of  AbstractSolutionData{Tsol, Tres}  must inherit the  Tsol  and  Tres  static parameters, and may have additional static parameters as well. It may also be helpful to define additional abstract types within the physics module to provide different levels of abstractions. For example, the Euler physics module defines:  abstract AbstractEulerData{Tsol, Tres}  : AbstractSolutionData{Tsol, Tres}\nabstract EulerData {Tsol, Tdim, Tres, var_type}  : AbstractEulerData{Tsol, Tres}\ntype EulerData_{Tsol, Tres, Tdim, Tmsh, var_type}  : EulerData{Tsol, Tdim, Tres, var_type}  The first line is effectively just a name change and may not be necessary. The second line adds the static parameters  Tdim , and  var_type  while inheriting the  Tsol  and  Tres  types from  AbstractEulerData .  Tdim  is the dimensionality of the equation,  Tres  is the datatype of the residual variables, and  var_type  is a symbol indicating whether the equation is being solved with conservative or entropy variables. The third line defines a concrete type that implements all the features required of an  AbstractSolutionData , and adds a static parameter  Tmsh , the datatype of the mesh variables.   The additional static parameter is necessary because one field of  EulerData_  has type  Tmsh . Note that there could be multiple implementations of  AbstractSolutionData  for the Euler equations, perhaps with different fields to store certain data or not. All these implementations will need to have the static parameters  Tsol ,  Tdim ,  Tres , and  var_type , so  EulerData  is defined as an abstract type,  allowing all implementations to inherit from it. All high level functions involved in evaluating the residual will take in an argument of type  EulerData . Only when low level functions need to dispatch based on which implementation is  used would it take in an  EulerData_  or another implementation.", 
            "title": "AbstractSolutionData implementation"
        }, 
        {
            "location": "/solver/Readme/#variable-conversion", 
            "text": "Some equations can be written in different variables, and need to convert between them.  To do this, it is  function convertFromNaturalToWorkingVars{Tsol}(params::ParamType{2, :var_type},                qc::AbstractArray{Tsol,1}, qe::AbstractArray{Tsol,1})  that converts from the \"natural\" variables in which to write an equation to some other set of variables at a node.  For the Euler equations, the \"natural\" variables would be the conservative variables, and one example of \"other\" variables would be the entropy variables.  It is also sometimes useful to define the opposite conversion, ie. from the working variables to the natural variables.", 
            "title": "Variable Conversion"
        }, 
        {
            "location": "/solver/Readme/#input-options", 
            "text": "Many of the components of PDESolver have different options that control how they work and what they do. In order to  provide a unified method of specifying these options, an dictionary  of type  Dict{ASCIIString, Any}  is read in from a disk file. This dictionary (called  opts  in function signatures), is passed to all high and mid level functions so they can use values in the dictionary to determine their  control flow. Low level functions need to be extremely efficient, so they cannot have conditional logic, therefore they are not passed the dictionary. Note that retrieving values from a dictionary is very slow compared to accessing the fields of a type, so all values that are accessed repeatedly should be stored  as the field of a type.", 
            "title": "Input Options"
        }, 
        {
            "location": "/solver/Readme/#functors", 
            "text": "Functors are a trick used to get Julia's dispatch system to make decisions at compile time rather than runtime.  This is particularly useful for boundary conditions, where the list of mesh faces that have boundary conditions applied is determined at runtime, but having conditional statements that execute for every node on the mesh boundary would be slow.  Instead a construct is used as follows:  type myBC  : BCType  # create a singleton type\nend\n\nfunction call(obj::myBC, q::AbstractVector, bndryflux::AbstractVector)\n  # calculate boundary flux here\nend  This defines a datatype and adds a method to the  call  function for that type. The call function is what makes a datatype callable like a function.  This method is called as follows:  functor = myBC()  # construct and object of type myBC\nq = rand(4)\nbndryflux = zeros(4)\nfunctor(q, bndryflux)  # the Julia compiler turns this into call(functor, q, bndryflux)    The way this is used for boundary conditions is through a two level construct where an outer function passes a functor to an inner function.  Julia's JIT will generate a method of the inner function that is specialized to the functor (this is why it is important that the functor is a datatype).  For example:  function getBCFluxes(mesh, sbp, eqn, opts)\n\n  for i=1:mesh.numBC  # loop over different boundary conditions\n    functor_i = mesh.bndry_functor[i]  # get the functor for this boundary condition\n    start_index = mesh.bndry_offsets[i]\n    end_index = mesh.bndry_offsets[i+1] - 1\n    # get data for boundary faces start_index:end_index\n\n    calcBoundaryFlux(functor_i, data for boundary faces start_index:end_index)\n  end\nend  # end function\n\n  function calcBoundaryFlux(functor_i::BCType, data for boundary faces start_index:end_index)\n    for i=1:length(start_index:end_index)\n      for j=1:num_nodes_on_face\n        # get data for this boundary face node\n        functor_i(data for this boundary face node)\n      end\n    end\n\n  end  # end function  The benefit of this arrangement is that  mesh.numBC  different version of calcBoundaryFlux get compiled, one for each functor, and each version knows about the  call  method that was defined for the functor it is passed.  This two level scheme allows the compiler to make all the decisions about what function to call (ie. the  call  method of the functor), avoiding any conditional logic at runtime  This idea is also applicable to the flux functions used by DG methods.", 
            "title": "Functors"
        }, 
        {
            "location": "/solver/Readme/#initialization-of-a-simulation", 
            "text": "This section lists an outline of how a simulation gets launched After step 4, the procedure becomes a bit more complicated because there are optional steps. Only the required steps are listed below.    The options dictionary is read in.  Default values are supplied for any key that is not specified, if a reasonable default value exists.    Second, the  sbp  operator is constructed.    The  mesh  object is constructed, using the options dictionary and the  sbp  operator.  Some of the options in the dictionary are used to determine how the mesh gets constructed.  For example, the options dictionary specifies what kind of mesh coloring to do.    The  eqn  object is constructed, using the  mesh ,  sbp , and  opts  objects.    The physics module  init  function is called, which initializes the physics module and finishes any initialization that  mesh  and  eqn  objects require.    The initial condition is applied to  eqn.q_vec .    A nonlinear solver is called.  Which solver is called and what parameters it uses are determined by the options dictionary.    Post-processing is done, if required by the options dictionary.", 
            "title": "Initialization of a Simulation"
        }, 
        {
            "location": "/solver/advection/advection/", 
            "text": "Advection Physics Documentation\n\n\nDescribe the equation being solved here\n\n\n\n\nAdvection Physics Documentation\n\n\nAdvection Types\n\n\nAdvection Volume Integrals\n\n\nAdvection Face Integrals\n\n\nAdvection Boundary Integrals\n\n\nAdvection Initial Conditions\n\n\nAdvection Source Term\n\n\nAdvection Common Functions\n\n\nAdvection Adjoint\n\n\nAdvection Boundary Functional", 
            "title": "Introduction"
        }, 
        {
            "location": "/solver/advection/advection/#advection-physics-documentation", 
            "text": "Describe the equation being solved here   Advection Physics Documentation  Advection Types  Advection Volume Integrals  Advection Face Integrals  Advection Boundary Integrals  Advection Initial Conditions  Advection Source Term  Advection Common Functions  Advection Adjoint  Advection Boundary Functional", 
            "title": "Advection Physics Documentation"
        }, 
        {
            "location": "/solver/advection/types/", 
            "text": "Advection Types", 
            "title": "Datatypes"
        }, 
        {
            "location": "/solver/advection/types/#advection-types", 
            "text": "", 
            "title": "Advection Types"
        }, 
        {
            "location": "/solver/advection/volume/", 
            "text": "Advection Volume Integrals", 
            "title": "Volume Integrals"
        }, 
        {
            "location": "/solver/advection/volume/#advection-volume-integrals", 
            "text": "", 
            "title": "Advection Volume Integrals"
        }, 
        {
            "location": "/solver/advection/flux/", 
            "text": "Advection Face Integrals", 
            "title": "Face Integrals"
        }, 
        {
            "location": "/solver/advection/flux/#advection-face-integrals", 
            "text": "", 
            "title": "Advection Face Integrals"
        }, 
        {
            "location": "/solver/advection/bc/", 
            "text": "Advection Boundary Integrals", 
            "title": "Boundary Integrals"
        }, 
        {
            "location": "/solver/advection/bc/#advection-boundary-integrals", 
            "text": "", 
            "title": "Advection Boundary Integrals"
        }, 
        {
            "location": "/solver/advection/ic/", 
            "text": "Advection Initial Conditions", 
            "title": "Initial Conditions"
        }, 
        {
            "location": "/solver/advection/ic/#advection-initial-conditions", 
            "text": "", 
            "title": "Advection Initial Conditions"
        }, 
        {
            "location": "/solver/advection/source/", 
            "text": "Advection Source Term", 
            "title": "Source Term"
        }, 
        {
            "location": "/solver/advection/source/#advection-source-term", 
            "text": "", 
            "title": "Advection Source Term"
        }, 
        {
            "location": "/solver/advection/common/", 
            "text": "Advection Common Functions", 
            "title": "Common Functions"
        }, 
        {
            "location": "/solver/advection/common/#advection-common-functions", 
            "text": "", 
            "title": "Advection Common Functions"
        }, 
        {
            "location": "/solver/advection/adjoint/", 
            "text": "Advection Adjoint", 
            "title": "Adjoint"
        }, 
        {
            "location": "/solver/advection/adjoint/#advection-adjoint", 
            "text": "", 
            "title": "Advection Adjoint"
        }, 
        {
            "location": "/solver/advection/boundary_functional/", 
            "text": "Advection Boundary Functional", 
            "title": "Boundary Functional"
        }, 
        {
            "location": "/solver/advection/boundary_functional/#advection-boundary-functional", 
            "text": "", 
            "title": "Advection Boundary Functional"
        }, 
        {
            "location": "/solver/euler/euler/", 
            "text": "Euler Physics Documentation\n\n\nDescribe the equation being solved here\n\n\n\n\nEuler Datatype Documentation\n\n\nVolume Integrals\n\n\nFace Integral\n\n\nBoundary Integrals\n\n\nInitial Conditions\n\n\nSource Term\n\n\nCommon Functions\n\n\nConversion Between Different Variables\n\n\nNumerical Flux Functions\n\n\nStabilization Documentation\n\n\nAdjoint\n\n\nBoundary Functional\n\n\nMiscellaneous Function", 
            "title": "Introduction"
        }, 
        {
            "location": "/solver/euler/euler/#euler-physics-documentation", 
            "text": "Describe the equation being solved here   Euler Datatype Documentation  Volume Integrals  Face Integral  Boundary Integrals  Initial Conditions  Source Term  Common Functions  Conversion Between Different Variables  Numerical Flux Functions  Stabilization Documentation  Adjoint  Boundary Functional  Miscellaneous Function", 
            "title": "Euler Physics Documentation"
        }, 
        {
            "location": "/solver/euler/types/", 
            "text": "Euler Datatype Documentation", 
            "title": "Datatypes"
        }, 
        {
            "location": "/solver/euler/types/#euler-datatype-documentation", 
            "text": "", 
            "title": "Euler Datatype Documentation"
        }, 
        {
            "location": "/solver/euler/volume/", 
            "text": "Volume Integrals", 
            "title": "Volume Integrals"
        }, 
        {
            "location": "/solver/euler/volume/#volume-integrals", 
            "text": "", 
            "title": "Volume Integrals"
        }, 
        {
            "location": "/solver/euler/flux/", 
            "text": "Face Integral", 
            "title": "Face Integrals"
        }, 
        {
            "location": "/solver/euler/flux/#face-integral", 
            "text": "", 
            "title": "Face Integral"
        }, 
        {
            "location": "/solver/euler/bc/", 
            "text": "Boundary Integrals", 
            "title": "Boundary Integrals"
        }, 
        {
            "location": "/solver/euler/bc/#boundary-integrals", 
            "text": "", 
            "title": "Boundary Integrals"
        }, 
        {
            "location": "/solver/euler/ic/", 
            "text": "Initial Conditions", 
            "title": "Initial Conditions"
        }, 
        {
            "location": "/solver/euler/ic/#initial-conditions", 
            "text": "", 
            "title": "Initial Conditions"
        }, 
        {
            "location": "/solver/euler/source/", 
            "text": "Source Term", 
            "title": "Source Term"
        }, 
        {
            "location": "/solver/euler/source/#source-term", 
            "text": "", 
            "title": "Source Term"
        }, 
        {
            "location": "/solver/euler/common/", 
            "text": "Common Functions", 
            "title": "Common Functions"
        }, 
        {
            "location": "/solver/euler/common/#common-functions", 
            "text": "", 
            "title": "Common Functions"
        }, 
        {
            "location": "/solver/euler/conversion/", 
            "text": "Conversion Between Different Variables", 
            "title": "Conversion"
        }, 
        {
            "location": "/solver/euler/conversion/#conversion-between-different-variables", 
            "text": "", 
            "title": "Conversion Between Different Variables"
        }, 
        {
            "location": "/solver/euler/flux_functions/", 
            "text": "Numerical Flux Functions\n\n\nbc_solvers.jl should be renamed to this", 
            "title": "Numerical Flux Functions"
        }, 
        {
            "location": "/solver/euler/flux_functions/#numerical-flux-functions", 
            "text": "bc_solvers.jl should be renamed to this", 
            "title": "Numerical Flux Functions"
        }, 
        {
            "location": "/solver/euler/stabilization/", 
            "text": "Stabilization Documentation", 
            "title": "Stabilization"
        }, 
        {
            "location": "/solver/euler/stabilization/#stabilization-documentation", 
            "text": "", 
            "title": "Stabilization Documentation"
        }, 
        {
            "location": "/solver/euler/adjoint/", 
            "text": "Adjoint", 
            "title": "Adjoint"
        }, 
        {
            "location": "/solver/euler/adjoint/#adjoint", 
            "text": "", 
            "title": "Adjoint"
        }, 
        {
            "location": "/solver/euler/boundary_functional/", 
            "text": "Boundary Functional", 
            "title": "Boundary Functional"
        }, 
        {
            "location": "/solver/euler/boundary_functional/#boundary-functional", 
            "text": "", 
            "title": "Boundary Functional"
        }, 
        {
            "location": "/solver/euler/misc/", 
            "text": "Miscellaneous Function\n\n\nA bunch of the things in euler_funcs.jl", 
            "title": "Misc"
        }, 
        {
            "location": "/solver/euler/misc/#miscellaneous-function", 
            "text": "A bunch of the things in euler_funcs.jl", 
            "title": "Miscellaneous Function"
        }, 
        {
            "location": "/solver/simpleODE/simpleODE/", 
            "text": "Simple ODE Documentation", 
            "title": "Simple ODE"
        }, 
        {
            "location": "/solver/simpleODE/simpleODE/#simple-ode-documentation", 
            "text": "", 
            "title": "Simple ODE Documentation"
        }, 
        {
            "location": "/input/input/", 
            "text": "Input Module Documentation", 
            "title": "Input"
        }, 
        {
            "location": "/input/input/#input-module-documentation", 
            "text": "", 
            "title": "Input Module Documentation"
        }, 
        {
            "location": "/NonlinearSolvers/nonlinearsolvers/", 
            "text": "NonlinearSolvers Documentation", 
            "title": "Introduction"
        }, 
        {
            "location": "/NonlinearSolvers/nonlinearsolvers/#nonlinearsolvers-documentation", 
            "text": "", 
            "title": "NonlinearSolvers Documentation"
        }, 
        {
            "location": "/NonlinearSolvers/steady/", 
            "text": "Steady NonlinearSolver Documentation", 
            "title": "Steady"
        }, 
        {
            "location": "/NonlinearSolvers/steady/#steady-nonlinearsolver-documentation", 
            "text": "", 
            "title": "Steady NonlinearSolver Documentation"
        }, 
        {
            "location": "/NonlinearSolvers/unsteady/", 
            "text": "Unsteady NonlinearSolver Documentation", 
            "title": "Unsteady"
        }, 
        {
            "location": "/NonlinearSolvers/unsteady/#unsteady-nonlinearsolver-documentation", 
            "text": "", 
            "title": "Unsteady NonlinearSolver Documentation"
        }, 
        {
            "location": "/Utils/Utils/", 
            "text": "Utilties\n\n\nThis module contains functions and types that are useful for the solver but independent the equation being solved. Additional utility functions are located in the \nODLCommontools\n. The functions defined in the \nUtils\n module are useful in the context of \nPDESolver\n and depend on the functions and datatypes defined in the other parts of the solver. The functions defined in \nODLCommonTools\n are more general in nature and usable independent of \nPDESolver\n.\n\n\n\n\nInput/Output\n\n\nLogging Documentation\n\n\nProjections documentation\n\n\nParallel Constructs Documentations", 
            "title": "Introduction"
        }, 
        {
            "location": "/Utils/Utils/#utilties", 
            "text": "This module contains functions and types that are useful for the solver but independent the equation being solved. Additional utility functions are located in the  ODLCommontools . The functions defined in the  Utils  module are useful in the context of  PDESolver  and depend on the functions and datatypes defined in the other parts of the solver. The functions defined in  ODLCommonTools  are more general in nature and usable independent of  PDESolver .   Input/Output  Logging Documentation  Projections documentation  Parallel Constructs Documentations", 
            "title": "Utilties"
        }, 
        {
            "location": "/Utils/parallel/", 
            "text": "Parallel Constructs Documentations\n\n\nThese function define the primative operations used by the physics modules to exchange data in parallel. When using these functions, the should not have to make any MPI calls directly, they should all be encapsulated within the provided functions.\n\n\nTODO: crossref to physics module documentation\n\n\nThe \nTypes and Basic API\n section describes the \nSharedFaceData\n datatype and the basic functions that operate on it. The \nParallel Data Exchange\n section describes the functions used by the physics modules that start and finish parallel parallel communication.\n\n\n\n\nTypes and Basic API\n\n\n#\n\n\nUtils.SharedFaceData\n \n \nType\n.\n\n\nThis type holds all the data necessary to perform MPI communication with   a given peer process that shared mesh edges (2D) or faces (3D) with the   current process.\n\n\nFields:\n\n\npeernum: the MPI rank of the peer process\npeeridx: the index of this peer in mesh.peer_parts\nmyrank: MPI rank of the current process\ncomm: MPI communicator used to define the above\n\nq_send: the send buffer, a 3D array of n x m x d.  While these dimensions\n        are arbitrary, there are two commonly used case.  If\n        opts[\nparallel_type\n] == face, then m is mesh.numNodesPerFace and\n        d is the number of faces shared with peernum.\n        If opts[\nparallel_type\n] == element, then \n        m = mesh.numNodesPerElement and d is the number of elements that\n        share faces with peernum.\nq_recv: the receive buffer.  Similar to q_send, except the size needs to\n        to be the number of entities on the *remote* process.\n\nsend_waited: has someone called MPI.Wait() on send_req yet?  Some MPI\n             implementations complain if Wait() is called on a Request\n             more than once, so use this field to avoid doing so.\nsend_req: the MPI.Request object for the Send/Isend/whatever other type of\n          Send\nsend_status: the MPI.Status object returned by calling Wait() on send_req\n\nrecv_waited: like send_waited, but for the receive\nrecv_req: like send_req, but for the receive\nrecv_status: like send_status, but for the receive\n\nbndries_local: Vector of Boundaries describing the faces from the local\n               side of the interface\nbndries_remote: Vector of Boundaries describing the facaes from the remote\n                side (see the documentation for PdePumiInterface before\n                using this field)\ninterfaces: Vector of Interfaces describing the faces from both sides (see\n            the documentation for PdePumiInterfaces, particularly the\n            mesh.shared_interfaces field, before using this field\n\n\n\n\n#\n\n\nUtils.SharedFaceData\n \n \nMethod\n.\n\n\nOuter constructor for SharedFaceData.\n\n\nInputs:\n\n\nmesh: a mesh object\npeeridx: the index of a peer in mesh.peer_parts\nq_send: the send buffer\nq_recv: the receive buffer\n\n\n\n\n#\n\n\nUtils.getSharedFaceData\n \n \nMethod\n.\n\n\nThis function returns a vector of SharedFaceData objects, one for each   peer processes the current process shares mesh edge (2D) or face (3D) with.   This function is intended to be used by the AbstractSolutionData constructors,   although it can be used to create additional vectors of SharedFaceData   objects.\n\n\nif opts[\"parallel_data\"] == \"face\", then the send and receive buffers   are numDofPerNode x numNodesPerFace x number of shared faces.\n\n\nif opts[\"parallel_data\"] == \"element\", the send and receive buffers are     numDofPerNode x numNodesPerElement x number of elements that share the     faces.  Note that the number of elements that share the faces can be     different for the send and receive buffers.\n\n\nInputs:\n\n\nTsol: element type of the arrays\nmesh: an AbstractMesh object\nsbp: an SBP operator\nopts: the options dictonary\n\n\n\n\nOutputs:\n\n\ndata_vec: Vector{SharedFaceData}.  data_vec[i] corresponds to \n          mesh.peer_parts[i]\n\n\n\n\n#\n\n\nBase.copy!\n \n \nMethod\n.\n\n\nIn-place copy for SharedFaceData.  This copies the buffers, but does not   retain the state of the Request and Status fields.  Instead they are   initialized the same as the constructor.\n\n\nThis function may only be called after receiving is complete,   otherwise an exception is thrown.\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\nCopy function for SharedFaceData.  Note that this does \nnot\n retain the   send_req/send_status (and similarly for the recceive) state   of the original object.  Instead, they are initialized the same as the   constructor.\n\n\nThis function may only be called after receiving is complete,   otherwise an exception is thrown.\n\n\n#\n\n\nUtils.assertReceivesConsistent\n \n \nMethod\n.\n\n\nLike assertSendsConsistent, but for the receives\n\n\n#\n\n\nUtils.assertReceivesWaited\n \n \nMethod\n.\n\n\nThis function verifies all the receives have been waited on for the    supplied SharedFaceData objects\n\n\n#\n\n\nUtils.assertSendsConsistent\n \n \nMethod\n.\n\n\nVerify either all or none of the sends have been waited on.  Throw an   exception otherwise.\n\n\nInputs:\n\n\nshared_data: Vector of SharedFaceData objects\n\n\n\n\nOutput:\n\n\nval: number of receives that have been waited on\n\n\n\n\n#\n\n\nUtils.waitAllReceives\n \n \nMethod\n.\n\n\nThis function is like MPI.Waitall, operating on the recvs of a vector of    SharedFaceData objects\n\n\n#\n\n\nUtils.waitAllSends\n \n \nMethod\n.\n\n\nThis function is like MPI.Waitall, operating on the sends of a vector of    SharedFaceData objects\n\n\n#\n\n\nUtils.waitAnyReceive\n \n \nMethod\n.\n\n\nLike MPI.WaitAny, but operates on the receives of  a vector of SharedFaceData.   Only the index of the Request that was waited on is returned,    the Status and recv_waited fields of hte SharedFaceData are updated internally\n\n\n\n\nParallel Data Exchange\n\n\nThe functions in this section are used to start sending data in parallel and finish receiving it. All functions operate on a \nVector\n of \nSharedFaceData\n that define what data to send to which peer processes.  See \nParallel Overview\n for a high-level overview of how the code is parallelized.\n\n\nSending the data to the other processes is straight-forward.  Receiving it (efficiently) is not. In particular, [\nfinishExchangeData\n] waits to receive data from one peer process, calls a user supplied callback function to do calculations involving the received data, and then waits for the next receive to finish. This is significantly more efficient than waiting for all receives to finish and then doing computations on all the data.\n\n\nThis section describes the API the physics modules use to do parallel  communication.  The \nInternals\n section describes the helper functions used in the implementation.\n\n\n#\n\n\nUtils.startSolutionExchange\n \n \nFunction\n.\n\n\nThis function is a thin wrapper around exchangeData().  It is used for the   common case of sending and receiving the solution variables to other processes.   It uses eqn.shared_data to do the parallel communication.   eqn.shared_data \nmust\n be passed into the corresponding finishDataExchange   call.\n\n\nInputs:     mesh: an AbstractMesh     sbp: an SBP operator     eqn: an AbstractSolutionData     opts: options dictionary\n\n\nKeyword arguments:     tag: MPI tag to use for communication, defaults to TAG_DEFAULT     wait: wait for sends and receives to finish before exiting\n\n\n#\n\n\nUtils.exchangeData\n \n \nFunction\n.\n\n\nThis function posts the MPI sends and receives for a vector of SharedFaceData.  It works for both opts[\"parallel_data\"] == \"face\" or \"element\".  The only   difference between these two cases is the populate_buffer() function.\n\n\nThe previous receives using these SharedFaceData objects should have   completed by the time this function is called.  An exception is throw   if this is not the case.\n\n\nThe previous sends are likely to have completed by the time this function   is called, but they are waited on if not.  This function might not perform   well if the previous sends have not completed.   #TODO: fix this using WaitAny\n\n\nInputs:     mesh: an AbstractMesh     sbp: an SBPOperator     eqn: an AbstractSolutionData     opts: the options dictionary     populate_buffer: function with the signature:                      populate_buffer(mesh, sbp, eqn, opts, data::SharedFaceData)                      that populates data.q_send   Inputs/Outputs:     shared_data: vector of SharedFaceData objects representing the parallel                  communication to be done\n\n\nKeyword Arguments:     tag: MPI tag to use for this communication, defaults to TAG_DEFAULT          This tag is typically used by the communication of the solution          variables to other processes.  Other users of this function should          provide their own tag\n\n\nwait: wait for the sends and receives to finish before returning.  This\n      is a debugging option only.  It will kill parallel performance.\n\n\n\n\n#\n\n\nUtils.finishExchangeData\n \n \nFunction\n.\n\n\nThis is the counterpart of exchangeData.  This function finishes the   receives started in exchangeData.\n\n\nThis function (efficiently) waits for a receive to finish and calls   a function to do calculations for on that data. If opts[\"parallel_data\"]   == \"face\", it also permutes the data in the receive buffers to agree   with the ordering of elementL.  For opts[\"parallel_data\"] == \"element\",   users should call SummationByParts.interiorFaceInterpolate to interpolate   the data to the face while ensuring proper permutation.\n\n\nInputs:     mesh: an AbstractMesh     sbp: an SBPOperator     eqn: an AbstractSolutionData     opts: the options dictonary     calc_func: function that does calculations for a set of shared faces                described by a single SharedFaceData.  It must have the signature                calc_func(mesh, sbp, eqn, opts, data::SharedFaceData)\n\n\nInputs/Outputs:     shared_data: vector of SharedFaceData, one for each peer process that                  needs to be communicated with.  By the time calc_func is                  called, the SharedFaceData passed to it has its q_recv field                  populated.  See note above about data permutation.\n\n\n#\n\n\nUtils.TAG_DEFAULT\n \n \nConstant\n.\n\n\nDefault MPI tag used for sending and receiving solution variables.\n\n\n\n\nInternals\n\n\nThese helper functions are used by the functions in \nParallel Data Exchange\n.\n\n\n#\n\n\nUtils.verifyReceiveCommunication\n \n \nFunction\n.\n\n\nUtils.verifyCommunication\n\n\nThis function checks the data provided by the Status object to verify a    communication completed successfully.  The sender's rank and the number of   elements is checked agains the expected sender and the buffer size\n\n\nInputs:     data: a SharedFaceData\n\n\n#\n\n\nUtils.getSendDataFace\n \n \nFunction\n.\n\n\nThis function populates the send buffer from eqn.q for    opts[\"parallle_data\"]  == \"face\"\n\n\nInputs:     mesh: a mesh     sbp: an SBP operator     eqn: an AbstractSolutionData     opts: options dictonary\n\n\nInputs/Outputs:     data: a SharedFaceData.  data.q_send will be overwritten\n\n\n#\n\n\nUtils.getSendDataElement\n \n \nFunction\n.\n\n\nThis function populates the send buffer from eqn.q for    opts[\"parallle_data\"]  == \"element\"\n\n\nInputs:\n\n\nmesh: a mesh\nsbp: an SBP operator\neqn: an AbstractSolutionData\nopts: options dictonary\n\n\n\n\nInputs/Outputs:\n\n\ndata: a SharedFaceData.  data.q_send will be overwritten\n\n\n\n\n#\n\n\nUtils.@mpi_master\n \n \nMacro\n.\n\n\nUtils.mpi_master\n\n\nThis macro introduces an if statement that causes the expression to be    executed only if the variable myrank is equal to zero.  myrank must exist   in the scope of the caller\n\n\n#\n\n\nUtils.@time_all\n \n \nMacro\n.\n\n\nUtils.time_all\n\n\nThis macro returns the value produced by the expression as well as    the execution time, the GC time, and the amount of memory allocated", 
            "title": "Parallel Constructs"
        }, 
        {
            "location": "/Utils/parallel/#parallel-constructs-documentations", 
            "text": "These function define the primative operations used by the physics modules to exchange data in parallel. When using these functions, the should not have to make any MPI calls directly, they should all be encapsulated within the provided functions.  TODO: crossref to physics module documentation  The  Types and Basic API  section describes the  SharedFaceData  datatype and the basic functions that operate on it. The  Parallel Data Exchange  section describes the functions used by the physics modules that start and finish parallel parallel communication.", 
            "title": "Parallel Constructs Documentations"
        }, 
        {
            "location": "/Utils/parallel/#types-and-basic-api", 
            "text": "#  Utils.SharedFaceData     Type .  This type holds all the data necessary to perform MPI communication with   a given peer process that shared mesh edges (2D) or faces (3D) with the   current process.  Fields:  peernum: the MPI rank of the peer process\npeeridx: the index of this peer in mesh.peer_parts\nmyrank: MPI rank of the current process\ncomm: MPI communicator used to define the above\n\nq_send: the send buffer, a 3D array of n x m x d.  While these dimensions\n        are arbitrary, there are two commonly used case.  If\n        opts[ parallel_type ] == face, then m is mesh.numNodesPerFace and\n        d is the number of faces shared with peernum.\n        If opts[ parallel_type ] == element, then \n        m = mesh.numNodesPerElement and d is the number of elements that\n        share faces with peernum.\nq_recv: the receive buffer.  Similar to q_send, except the size needs to\n        to be the number of entities on the *remote* process.\n\nsend_waited: has someone called MPI.Wait() on send_req yet?  Some MPI\n             implementations complain if Wait() is called on a Request\n             more than once, so use this field to avoid doing so.\nsend_req: the MPI.Request object for the Send/Isend/whatever other type of\n          Send\nsend_status: the MPI.Status object returned by calling Wait() on send_req\n\nrecv_waited: like send_waited, but for the receive\nrecv_req: like send_req, but for the receive\nrecv_status: like send_status, but for the receive\n\nbndries_local: Vector of Boundaries describing the faces from the local\n               side of the interface\nbndries_remote: Vector of Boundaries describing the facaes from the remote\n                side (see the documentation for PdePumiInterface before\n                using this field)\ninterfaces: Vector of Interfaces describing the faces from both sides (see\n            the documentation for PdePumiInterfaces, particularly the\n            mesh.shared_interfaces field, before using this field  #  Utils.SharedFaceData     Method .  Outer constructor for SharedFaceData.  Inputs:  mesh: a mesh object\npeeridx: the index of a peer in mesh.peer_parts\nq_send: the send buffer\nq_recv: the receive buffer  #  Utils.getSharedFaceData     Method .  This function returns a vector of SharedFaceData objects, one for each   peer processes the current process shares mesh edge (2D) or face (3D) with.   This function is intended to be used by the AbstractSolutionData constructors,   although it can be used to create additional vectors of SharedFaceData   objects.  if opts[\"parallel_data\"] == \"face\", then the send and receive buffers   are numDofPerNode x numNodesPerFace x number of shared faces.  if opts[\"parallel_data\"] == \"element\", the send and receive buffers are     numDofPerNode x numNodesPerElement x number of elements that share the     faces.  Note that the number of elements that share the faces can be     different for the send and receive buffers.  Inputs:  Tsol: element type of the arrays\nmesh: an AbstractMesh object\nsbp: an SBP operator\nopts: the options dictonary  Outputs:  data_vec: Vector{SharedFaceData}.  data_vec[i] corresponds to \n          mesh.peer_parts[i]  #  Base.copy!     Method .  In-place copy for SharedFaceData.  This copies the buffers, but does not   retain the state of the Request and Status fields.  Instead they are   initialized the same as the constructor.  This function may only be called after receiving is complete,   otherwise an exception is thrown.  #  Base.copy     Method .  Copy function for SharedFaceData.  Note that this does  not  retain the   send_req/send_status (and similarly for the recceive) state   of the original object.  Instead, they are initialized the same as the   constructor.  This function may only be called after receiving is complete,   otherwise an exception is thrown.  #  Utils.assertReceivesConsistent     Method .  Like assertSendsConsistent, but for the receives  #  Utils.assertReceivesWaited     Method .  This function verifies all the receives have been waited on for the    supplied SharedFaceData objects  #  Utils.assertSendsConsistent     Method .  Verify either all or none of the sends have been waited on.  Throw an   exception otherwise.  Inputs:  shared_data: Vector of SharedFaceData objects  Output:  val: number of receives that have been waited on  #  Utils.waitAllReceives     Method .  This function is like MPI.Waitall, operating on the recvs of a vector of    SharedFaceData objects  #  Utils.waitAllSends     Method .  This function is like MPI.Waitall, operating on the sends of a vector of    SharedFaceData objects  #  Utils.waitAnyReceive     Method .  Like MPI.WaitAny, but operates on the receives of  a vector of SharedFaceData.   Only the index of the Request that was waited on is returned,    the Status and recv_waited fields of hte SharedFaceData are updated internally", 
            "title": "Types and Basic API"
        }, 
        {
            "location": "/Utils/parallel/#parallel-data-exchange", 
            "text": "The functions in this section are used to start sending data in parallel and finish receiving it. All functions operate on a  Vector  of  SharedFaceData  that define what data to send to which peer processes.  See  Parallel Overview  for a high-level overview of how the code is parallelized.  Sending the data to the other processes is straight-forward.  Receiving it (efficiently) is not. In particular, [ finishExchangeData ] waits to receive data from one peer process, calls a user supplied callback function to do calculations involving the received data, and then waits for the next receive to finish. This is significantly more efficient than waiting for all receives to finish and then doing computations on all the data.  This section describes the API the physics modules use to do parallel  communication.  The  Internals  section describes the helper functions used in the implementation.  #  Utils.startSolutionExchange     Function .  This function is a thin wrapper around exchangeData().  It is used for the   common case of sending and receiving the solution variables to other processes.   It uses eqn.shared_data to do the parallel communication.   eqn.shared_data  must  be passed into the corresponding finishDataExchange   call.  Inputs:     mesh: an AbstractMesh     sbp: an SBP operator     eqn: an AbstractSolutionData     opts: options dictionary  Keyword arguments:     tag: MPI tag to use for communication, defaults to TAG_DEFAULT     wait: wait for sends and receives to finish before exiting  #  Utils.exchangeData     Function .  This function posts the MPI sends and receives for a vector of SharedFaceData.  It works for both opts[\"parallel_data\"] == \"face\" or \"element\".  The only   difference between these two cases is the populate_buffer() function.  The previous receives using these SharedFaceData objects should have   completed by the time this function is called.  An exception is throw   if this is not the case.  The previous sends are likely to have completed by the time this function   is called, but they are waited on if not.  This function might not perform   well if the previous sends have not completed.   #TODO: fix this using WaitAny  Inputs:     mesh: an AbstractMesh     sbp: an SBPOperator     eqn: an AbstractSolutionData     opts: the options dictionary     populate_buffer: function with the signature:                      populate_buffer(mesh, sbp, eqn, opts, data::SharedFaceData)                      that populates data.q_send   Inputs/Outputs:     shared_data: vector of SharedFaceData objects representing the parallel                  communication to be done  Keyword Arguments:     tag: MPI tag to use for this communication, defaults to TAG_DEFAULT          This tag is typically used by the communication of the solution          variables to other processes.  Other users of this function should          provide their own tag  wait: wait for the sends and receives to finish before returning.  This\n      is a debugging option only.  It will kill parallel performance.  #  Utils.finishExchangeData     Function .  This is the counterpart of exchangeData.  This function finishes the   receives started in exchangeData.  This function (efficiently) waits for a receive to finish and calls   a function to do calculations for on that data. If opts[\"parallel_data\"]   == \"face\", it also permutes the data in the receive buffers to agree   with the ordering of elementL.  For opts[\"parallel_data\"] == \"element\",   users should call SummationByParts.interiorFaceInterpolate to interpolate   the data to the face while ensuring proper permutation.  Inputs:     mesh: an AbstractMesh     sbp: an SBPOperator     eqn: an AbstractSolutionData     opts: the options dictonary     calc_func: function that does calculations for a set of shared faces                described by a single SharedFaceData.  It must have the signature                calc_func(mesh, sbp, eqn, opts, data::SharedFaceData)  Inputs/Outputs:     shared_data: vector of SharedFaceData, one for each peer process that                  needs to be communicated with.  By the time calc_func is                  called, the SharedFaceData passed to it has its q_recv field                  populated.  See note above about data permutation.  #  Utils.TAG_DEFAULT     Constant .  Default MPI tag used for sending and receiving solution variables.", 
            "title": "Parallel Data Exchange"
        }, 
        {
            "location": "/Utils/parallel/#internals", 
            "text": "These helper functions are used by the functions in  Parallel Data Exchange .  #  Utils.verifyReceiveCommunication     Function .  Utils.verifyCommunication  This function checks the data provided by the Status object to verify a    communication completed successfully.  The sender's rank and the number of   elements is checked agains the expected sender and the buffer size  Inputs:     data: a SharedFaceData  #  Utils.getSendDataFace     Function .  This function populates the send buffer from eqn.q for    opts[\"parallle_data\"]  == \"face\"  Inputs:     mesh: a mesh     sbp: an SBP operator     eqn: an AbstractSolutionData     opts: options dictonary  Inputs/Outputs:     data: a SharedFaceData.  data.q_send will be overwritten  #  Utils.getSendDataElement     Function .  This function populates the send buffer from eqn.q for    opts[\"parallle_data\"]  == \"element\"  Inputs:  mesh: a mesh\nsbp: an SBP operator\neqn: an AbstractSolutionData\nopts: options dictonary  Inputs/Outputs:  data: a SharedFaceData.  data.q_send will be overwritten  #  Utils.@mpi_master     Macro .  Utils.mpi_master  This macro introduces an if statement that causes the expression to be    executed only if the variable myrank is equal to zero.  myrank must exist   in the scope of the caller  #  Utils.@time_all     Macro .  Utils.time_all  This macro returns the value produced by the expression as well as    the execution time, the GC time, and the amount of memory allocated", 
            "title": "Internals"
        }, 
        {
            "location": "/Utils/projections/", 
            "text": "Projections documentation\n\n\nThe functions here project the vector of conservative variables back and  forth between x-y-z and n-t-b (normal-tangential-binormal) cordinates.\n\n\n\n\nDetailed Documentation\n\n\n#\n\n\nUtils.calcLength\n \n \nMethod\n.\n\n\nCalculates the length of a vector, using a manually unrolled loop.   Methods are available for 2D and 3D.\n\n\nInputs:\n\n\nparams: an AbstractParamType{Tdim}, used to dispatch to the right method\nnrm: the vector to calculate the length of.  Must have length 2 in 2D\n     and 3 in 3D\n\n\n\n\nOutputs:\n\n\nlength of nrm\n\n\n\n\n#\n\n\nUtils.getProjectionMatrix\n \n \nMethod\n.\n\n\nThis function populates the matrix P that project from x-y-z to n-t-b.   Methods are available for 2D and 3D, determined from the AbstractParamType   object.  This is a somewhat specialized routine in that it only works for   vectors like the state vector of the Euler or Navier-Stokes equations,   where the first and last equations are coordinate system invarient and   only the middle 2 (in 2D) or 3 (in 3D) equations need to be rotated.   Specialized multiplication routines are provied as projectToXY and   projectToNT.\n\n\nInputs:\n\n\nparams:  An AbstractParamType{Tdim}\nnrm: the normal direction in x-y coordinate system.  Must be a unit vector\n\n\n\n\nInputs/Outputs\n\n\nP:  matrix to be populated with projection matrix\n\n\n\n\nAliasing restrictions: none\n\n\n#\n\n\nUtils.projectToNT\n \n \nMethod\n.\n\n\nThis function projects a vector x from x-y coordinates to normal-tangential   (n-t) coordinates, where A is a projection matrix from x-y to n-t, obtained   from getProjectionMatrix.  This function is a specialized matrix-vector   multiplication routine and only works for matrices with the particular   sparsity pattern created by getProjectionMatrix.   Methods are available for 2D and 3D\n\n\nInputs:\n\n\nparams: an AbstractParamType{{Tdim} used to dispatch to the 2D or 3D\n        method\nP: the projection matrix\nx: vector to be projected\n\n\n\n\nInputs/Outputs:\n\n\nb: the result of the projection\n\n\n\n\nAliasing restrictions: x and b cannot alias\n\n\n#\n\n\nUtils.projectToXY\n \n \nMethod\n.\n\n\nThis function is similar to projectToNT, except it project from n-t   to x-y.  Note that P is still the projection matrix from getProjectionMatrix   that projects from x-y to n-t.\n\n\n#\n\n\nUtils.getBinormalVector\n \n \nMethod\n.\n\n\nThis function computes a vector normal to the 2 supplied vectors and   returns the components.\n\n\nInputs:\n\n\nn1, n2, n3: the components of the first vector\nt1, t2, t3: the components of the second vector\n\n\n\n\nOutputs:\n\n\nb1, b2, b3: the components of the binormal vector\n\n\n\n\nAliasing restrictions: none\n\n\n#\n\n\nUtils.getOrthogonalVector\n \n \nMethod\n.\n\n\nThis function generates a unit vector orthogonal to the input vector nrm and    returns its components.  Methods are availble for 2D and 3D\n\n\nInputs:\n\n\nparams: an AbstractParamType, used to dispatch to the 2D or 3D method\nnrm: the input vector\n\n\n\n\nOutputs:\n\n\nt1, t2, (and t3 in 3D): the components of the unit vector orthogonal to\n                        nrm\n\n\n\n\nAliasing restrictions: none", 
            "title": "Projections"
        }, 
        {
            "location": "/Utils/projections/#projections-documentation", 
            "text": "The functions here project the vector of conservative variables back and  forth between x-y-z and n-t-b (normal-tangential-binormal) cordinates.", 
            "title": "Projections documentation"
        }, 
        {
            "location": "/Utils/projections/#detailed-documentation", 
            "text": "#  Utils.calcLength     Method .  Calculates the length of a vector, using a manually unrolled loop.   Methods are available for 2D and 3D.  Inputs:  params: an AbstractParamType{Tdim}, used to dispatch to the right method\nnrm: the vector to calculate the length of.  Must have length 2 in 2D\n     and 3 in 3D  Outputs:  length of nrm  #  Utils.getProjectionMatrix     Method .  This function populates the matrix P that project from x-y-z to n-t-b.   Methods are available for 2D and 3D, determined from the AbstractParamType   object.  This is a somewhat specialized routine in that it only works for   vectors like the state vector of the Euler or Navier-Stokes equations,   where the first and last equations are coordinate system invarient and   only the middle 2 (in 2D) or 3 (in 3D) equations need to be rotated.   Specialized multiplication routines are provied as projectToXY and   projectToNT.  Inputs:  params:  An AbstractParamType{Tdim}\nnrm: the normal direction in x-y coordinate system.  Must be a unit vector  Inputs/Outputs  P:  matrix to be populated with projection matrix  Aliasing restrictions: none  #  Utils.projectToNT     Method .  This function projects a vector x from x-y coordinates to normal-tangential   (n-t) coordinates, where A is a projection matrix from x-y to n-t, obtained   from getProjectionMatrix.  This function is a specialized matrix-vector   multiplication routine and only works for matrices with the particular   sparsity pattern created by getProjectionMatrix.   Methods are available for 2D and 3D  Inputs:  params: an AbstractParamType{{Tdim} used to dispatch to the 2D or 3D\n        method\nP: the projection matrix\nx: vector to be projected  Inputs/Outputs:  b: the result of the projection  Aliasing restrictions: x and b cannot alias  #  Utils.projectToXY     Method .  This function is similar to projectToNT, except it project from n-t   to x-y.  Note that P is still the projection matrix from getProjectionMatrix   that projects from x-y to n-t.  #  Utils.getBinormalVector     Method .  This function computes a vector normal to the 2 supplied vectors and   returns the components.  Inputs:  n1, n2, n3: the components of the first vector\nt1, t2, t3: the components of the second vector  Outputs:  b1, b2, b3: the components of the binormal vector  Aliasing restrictions: none  #  Utils.getOrthogonalVector     Method .  This function generates a unit vector orthogonal to the input vector nrm and    returns its components.  Methods are availble for 2D and 3D  Inputs:  params: an AbstractParamType, used to dispatch to the 2D or 3D method\nnrm: the input vector  Outputs:  t1, t2, (and t3 in 3D): the components of the unit vector orthogonal to\n                        nrm  Aliasing restrictions: none", 
            "title": "Detailed Documentation"
        }, 
        {
            "location": "/Utils/logging/", 
            "text": "Logging Documentation\n\n\nThe functions define here assist with debugging by logging some results to disk.\n\n\n\n\nDetailed Documentation\n\n\n#\n\n\nUtils.sharedFaceLogging\n \n \nMethod\n.\n\n\nUtils.sharedFaceLogging\n\n\nThis function writes files for the function that evalutes the shared face   integrals.  This function should only be called in debug mode.\n\n\nIf opts[\"writeqface\"] is true,  writes the q values at the face to a file    q_sharedface_i_myrank.dat, where i is the peer process index (not MPI rank)   and myrank is the MPI rank of the current process. \n\n\nIf opts[\"write_fluxface\"] is true, it writes eqn.flux_sharedface to a file   flux_sharedface_i_myrank.dat\n\n\nInputs:     mesh     sbp     eqn: and AbstractSolutionData     opts: options dictonary     data: SharedFaceData object     qL_arr: an array holding solution values at face nodes on the              left side of the interface,  numDofPerNode x numfacenodes x              numsharedfaces on this partition boundary     qR_arr: solution values at face nodes on the right side of the interface.             same shape as qL_arr\n\n\nalso, the eqn.flux_shared is used to write the face flux.  It is the same   shape as qL_arr\n\n\nAliasing restrictions: qL_arr, qR_arr, and eqn.flux_sharedface must not alias.", 
            "title": "Logging"
        }, 
        {
            "location": "/Utils/logging/#logging-documentation", 
            "text": "The functions define here assist with debugging by logging some results to disk.", 
            "title": "Logging Documentation"
        }, 
        {
            "location": "/Utils/logging/#detailed-documentation", 
            "text": "#  Utils.sharedFaceLogging     Method .  Utils.sharedFaceLogging  This function writes files for the function that evalutes the shared face   integrals.  This function should only be called in debug mode.  If opts[\"writeqface\"] is true,  writes the q values at the face to a file    q_sharedface_i_myrank.dat, where i is the peer process index (not MPI rank)   and myrank is the MPI rank of the current process.   If opts[\"write_fluxface\"] is true, it writes eqn.flux_sharedface to a file   flux_sharedface_i_myrank.dat  Inputs:     mesh     sbp     eqn: and AbstractSolutionData     opts: options dictonary     data: SharedFaceData object     qL_arr: an array holding solution values at face nodes on the              left side of the interface,  numDofPerNode x numfacenodes x              numsharedfaces on this partition boundary     qR_arr: solution values at face nodes on the right side of the interface.             same shape as qL_arr  also, the eqn.flux_shared is used to write the face flux.  It is the same   shape as qL_arr  Aliasing restrictions: qL_arr, qR_arr, and eqn.flux_sharedface must not alias.", 
            "title": "Detailed Documentation"
        }, 
        {
            "location": "/Utils/io/", 
            "text": "Input/Output\n\n\nThe functions and types defined here facilitate writing to \nSTDOUT\n and \nSTDERR\n as well as files. In particular, performance tests have shown that buffering is important when running in parallel. To this end, the \nBufferedIO\n type is introduced that stores output in an in-memory buffer before writing to an underlying stream. This can create some difficulty when debuging, because output written to a buffered stream is not immediately written to the underlying stream. In cases where precise control of output is needed, users should call \nflush\n to make sure all output is written to the underlying stream\n\n\n\n\nDetailed Documentation\n\n\n#\n\n\nUtils.BSTDERR\n \n \nConstant\n.\n\n\nBuffered version of STDERR.  This should \nalways\n be used instead of STDERR\n\n\n#\n\n\nUtils.BSTDOUT\n \n \nConstant\n.\n\n\nBuffered version of STDOUT.  This should \nalways\n be used instead of STDOUT\n\n\n#\n\n\nUtils.BufferedIO\n \n \nType\n.\n\n\nUtils.BufferedIO\n\n\nThis type provides a means to buffer IO in memory before writing it to a file.   Data written to the object is stored in the IOBuffer until flush() is called,    when the buffer is (efficiently) dumped into the file\n\n\n#\n\n\nUtils.BufferedIO\n \n \nType\n.\n\n\nUtils.BufferedIO\n\n\nConstructor for BufferedIO type.  Takes an IOStream and creates an IOBuffer.   If no IOStream is given, a dummy stream is created.  If a dummy stream is   used, flush must never be called on the returned BufferedIO object\n\n\nInputs:\n\n\nf: an IOStream object, defaults to a dummy stream\n\n\n\n\nOutputs:\n\n\na BufferedIO object\n\n\n\n\n#\n\n\nUtils.BufferedIO\n \n \nType\n.\n\n\nAlternative constructor for BufferedIO, emulating the open() function.   This function creates the underlying file using open() and then creates   a BufferedIO around it.\n\n\nInputs:\n\n\nfname: AbstractString, name of file to open\nmode: file open mode, see documentation of open(), defaults to append\n\n\n\n\nOutputs:\n\n\na BufferedIO object\n\n\n\n\n#\n\n\nBase.close\n \n \nMethod\n.\n\n\nBase\n function \nclose\n extended for BufferedIO\n\n\n#\n\n\nBase.flush\n \n \nMethod\n.\n\n\nBase\n function \nflush\n extended for BufferedIO", 
            "title": "Input/Output"
        }, 
        {
            "location": "/Utils/io/#inputoutput", 
            "text": "The functions and types defined here facilitate writing to  STDOUT  and  STDERR  as well as files. In particular, performance tests have shown that buffering is important when running in parallel. To this end, the  BufferedIO  type is introduced that stores output in an in-memory buffer before writing to an underlying stream. This can create some difficulty when debuging, because output written to a buffered stream is not immediately written to the underlying stream. In cases where precise control of output is needed, users should call  flush  to make sure all output is written to the underlying stream", 
            "title": "Input/Output"
        }, 
        {
            "location": "/Utils/io/#detailed-documentation", 
            "text": "#  Utils.BSTDERR     Constant .  Buffered version of STDERR.  This should  always  be used instead of STDERR  #  Utils.BSTDOUT     Constant .  Buffered version of STDOUT.  This should  always  be used instead of STDOUT  #  Utils.BufferedIO     Type .  Utils.BufferedIO  This type provides a means to buffer IO in memory before writing it to a file.   Data written to the object is stored in the IOBuffer until flush() is called,    when the buffer is (efficiently) dumped into the file  #  Utils.BufferedIO     Type .  Utils.BufferedIO  Constructor for BufferedIO type.  Takes an IOStream and creates an IOBuffer.   If no IOStream is given, a dummy stream is created.  If a dummy stream is   used, flush must never be called on the returned BufferedIO object  Inputs:  f: an IOStream object, defaults to a dummy stream  Outputs:  a BufferedIO object  #  Utils.BufferedIO     Type .  Alternative constructor for BufferedIO, emulating the open() function.   This function creates the underlying file using open() and then creates   a BufferedIO around it.  Inputs:  fname: AbstractString, name of file to open\nmode: file open mode, see documentation of open(), defaults to append  Outputs:  a BufferedIO object  #  Base.close     Method .  Base  function  close  extended for BufferedIO  #  Base.flush     Method .  Base  function  flush  extended for BufferedIO", 
            "title": "Detailed Documentation"
        }
    ]
}