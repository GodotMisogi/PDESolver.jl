# parallel communication primatives

# MPI tags
"""
  Default MPI tag used for sending and receiving solution variables.
"""
global const TAG_DEFAULT = 1
global const TAG_FACE = 1  # exchanging face data
global const TAG_ELEMENT = 2  # exchanging element data

#=
function initMPIStructures(mesh::AbstractMesh, opts)

  # set all requests to REQUEST_NULL, Status to match
  # the MPI standard requires a Wait on a REQUEST_NULL to return immediately
  for i=1:mesh.npeers
    mesh.send_reqs[i] = MPI.REQUEST_NULL
    mesh.recv_reqs[i] = MPI.REQUEST_NULL
    mesh.send_stats[i] = MPI.Wait!(mesh.send_reqs[i])
    mesh.recv_stats[i] = MPI.Wait!(mesh.recv_reqs[i])
    # do not wait on these requests
    mesh.recv_waited[i] = true 
    mesh.send_waited[i] = true
  end

  return nothing
end
=#

"""
  This function is a thin wrapper around exchangeData().  It is used for the
  common case of sending and receiving the solution variables to other processes.
  It uses eqn.shared_data to do the parallel communication.

  Inputs:
    mesh: an AbstractMesh
    sbp: an SBP operator
    eqn: an AbstractSolutionData
    opts: options dictionary

  Keyword arguments:
    tag: MPI tag to use for communication, defaults to TAG_DEFAULT
    wait: wait for sends and receives to finish before exiting
"""
function startSolutionExchange{T}(mesh::AbstractMesh, sbp::AbstractSBP,
                                  eqn::AbstractSolutionData, opts;
                                  tag=TAG_DEFAULT, wait=false)

  if opts["parallel_data"] == "face"
    populate_buffer = getSendDataFace
  elseif opts["parallel_data"] == "element"
    populate_buffer = getSendDataElement
  else
    throw(ErrorException("unsupported parallel_type = $(opts["parallel_data"])"))
  end

  exchangeData(mesh, sbp, eqn, opts, eqn.shared_data, populate_buffer, tag=tag, wait=wait)

  return nothing
end

#=
@doc """
  This function posts the sends and receives for the exchange parallel data
  for all types parallel data (face or element), interpolating if needed.
  On exit, the send_buff is populated with the data send.
  It waits for the previous send request to finish before
  posting the new send.  It does not wait for the previous receive.
  
  The Requests generated by the sends and receives are stored in
  mesh.recv_reqs and mesh.send_reqs.  If the Requests are waited on, the
  Status objects are stored in mesh.send_stats and mesh.recv_stats

  If mesh.commsize == 1, this function is a no-op.

  Inputs:
    mesh: a mesh object
    opts: options dictonary
    q: the 3D array of data, from which the data for the element to be 
       sent to other processes will be picked out

  Inputs/Outputs:
    send_buff: array of arrays to copy the send data into.  There must be 
               mesh.npeers arrays, each size(q, 1) x size(q, 2) x number of
               elements on the local side of the interface
    recv_buff: array of arrays to put the received data into.  There must be
               mesh.npeers array, each size(q,1 ) x size(q, 2) x number of
               elements on the other side of the interface

  Keyword Args
    wait=false: whether not wait for all communications to finish before
                exiting

  Aliasing: no aliasing allowed

"""
function startDataExchange{T, N}(mesh::AbstractMesh, opts, q::Abstract3DArray,
                                   send_buff::Array{Array{T, N}, 1},
                                   recv_buff::Array{Array{T, N}, 1}, 
                                   f::IO=STDOUT; wait=false)

  if mesh.commsize == 1
    return nothing
  end

  if opts["parallel_data"] == "face"
    # interpolate to face
    for i=1:mesh.npeers
      mesh.send_waited[i] = getSendData(mesh, opts, q, mesh.bndries_local[i], 
                            send_buff[i], mesh.send_reqs[i], 
                            mesh.send_waited[i])

    end

    # send it
    exchangeFaceData(mesh, opts, send_buff, recv_buff, wait=wait)

  elseif opts["parallel_data"] == "element"
    # call exchangeElementData, because it internally extracts the 
    # needed values
    exchangeElementData(mesh, opts, q, send_buff, recv_buff, f, wait=wait)
  end

  return nothing
end
=#

"""
  This function posts the MPI sends and receives for a vector of SharedFaceData.  It works for both opts["parallel_data"] == "face" or "element".  The only
  difference between these two cases is the populate_buffer() function.

  The previous receives using these SharedFaceData objects should have
  completed by the time this function is called.  An exception is throw
  if this is not the case.

  The previous sends are likely to have completed by the time this function
  is called, but they are waited on if not.  This function might not perform
  well if the previous sends have not completed.
  #TODO: fix this using WaitAny

  Inputs:
    mesh: an AbstractMesh
    sbp: an SBPOperator
    eqn: an AbstractSolutionData
    opts: the options dictionary
    populate_buffer: function with the signature:
                     populate_buffer(mesh, sbp, eqn, opts, data::SharedFaceData)
                     that populates data.q_send
  Inputs/Outputs:
    shared_data: vector of SharedFaceData objects representing the parallel
                 communication to be done

  Keyword Arguments:
    tag: MPI tag to use for this communication, defaults to TAG_DEFAULT
         This tag is typically used by the communication of the solution
         variables to other processes.  Other users of this function should
         provide their own tag

    wait: wait for the sends and receives to finish before returning.  This
          is a debugging option only.  It will kill parallel performance.
"""
function exchangeData{T}(mesh::AbstractMesh, sbp::AbstractSBP,
                         eqn::AbstractSolutionData, opts,
                         shared_data::Vector{SharedFaceData{T}},
                         populate_buffer::Function;
                         tag=TAG_DEFAULT, wait=false)

  npeers = length(shared_data)

  # bail out early if there is no communication to do
  # not sure if the rest of this function runs correctly if npeers == 0
  if npeers == 0
    return nothing
  end

  # this should already have happened.  If it hasn't something else has
  # gone wrong in the solver.  Throw an exception
  assertReceivesWaited(shared_data)

  # post the receives first
  for i=1:npeers
    data_i = shared_data[i]
    peer_i = data_i.peernum
    recv_buff = data_i.q_recv
    data_i.recv_req = MPI.Irecv!(recv_buff, peer_i, tag, data_i.comm)
    data_i.recv_waited = false
  end

  # verify the sends are consistent
  assertSendsConsistent(shared_data)

  for i=1:npeers
    # wait for these in order because doing the waitany trick doesn't work
    # these should have completed long ago, so it shouldn't be a performance
    # problem

    # the waitany trick doesn't work because this loop posts new sends, reusing
    # the same array of MPI_Requests.

    # TODO: use 2 arrays for the Requests: old and new, so the WaitAny trick
    #       works


    idx = i
    data_i = shared_data[idx]

    # wait on the previous send if it hasn't been waited on yet
    if !data_i.send_waited
      MPI.Wait!(data_i.send_req)
      data_i.send_waited = true
    end

    # move data to send buffer
    populate_buffer(mesh, sbp, eqn, opts, data_i)

    # post the send
    peer_i = data_i.peernum
    send_buff = data_i.q_send
    data_i.send_req = MPI.Isend(send_buff, peer_i, tag, data_i.comm)
    data_i.send_waited = false
  end


  if wait
    waitAllSends(shared_data)
    waitAllReceives(shared_data)
  end

  return nothing
end

"""
  This is the counterpart of exchangeData.  This function finishes the
  receives started in exchangeData.

  This function (efficiently) waits for a receive to finish and calls
  a function to do calculations for on that data.

  Inputs:
    mesh: an AbstractMesh
    sbp: an SBPOperator
    eqn: an AbstractSolutionData
    opts: the options dictonary
    calc_func: function that does calculations for a set of shared faces
               described by a single SharedFaceData.  It must have the signature
               calc_func(mesh, sbp, eqn, opts, data::SharedFaceData)

  Inputs/Outputs:
    shared_data: vector of SharedFaceData, one for each peer process that
                 needs to be communicated with.  By the time calc_func is
                 called, the SharedFaceData passed to it has its q_recv field
                 populated.  Note that the convention for Ticon is that the
                 receiver is responsible for permuting the received data if
                 needed (for example, in 2D the order of the face nodes needs
                 to be reversed).
"""
function finishExchangeData{T}(mesh, sbp, eqn, opts,
                               shared_data::Vector{SharedFaceData{T}},
                               calc_func::Function)

  npeers = length(shared_data)
  val = assertReceivesConsistent(shared_data)
  
  for i=1:npeers
    if val == 0  # request have not been waited on previously
      eqn.params.t_wait += @elapsed idx = waitAnyReceive(shared_data)
    else
      idx = i
    end

    calc_func(mesh, sbp, eqn, opts, shared_data[idx])

  end

  return nothing
end


@doc """
### Utils.exchangeFaceData
  
  This function performs communications with peer processes, sending and 
  receiving data from the shared faces.  Non-blocking sends and receives
  are posted and may or may not be waited on depending on the keyword
  arguments.  The Requests generated by the sends and receives are stored in
  mesh.recv_reqs and mesh.send_reqs.  If the Requests are waited on, the
  Status objects are stored in mesh.send_stats and mesh.recv_stats

  Inputs/Outputs:
    mesh:  an AbstractMesh
    opts:  options dictonary
    send_data: an array of arrays, where the number of arrays is the number
             of peers processes and the length of each array is the number of
             faces shared with that peer.  Tese arrays contain the data to be
             send to the other processes
    recv_data: same as in_data, except the data received from the peer processes
              will be stored here

  Keyword arguments:
    tag: the MPI tag, defaults to TAG_FACE
    wait: whether or not to wait on the Request objects, default false

  Aliasing Restrictions:  none of the arrays can alias each other
"""->
function exchangeFaceData{T, N}(mesh::AbstractMesh, opts, 
                         send_data::Array{Array{T, N}, 1}, 
                         recv_data::Array{Array{T, N}, 1}, f::IO=STDOUT; 
                         tag=TAG_FACE, wait=false)
# post sends and receives for face data exchange

  for i=1:mesh.npeers
    peer_i = mesh.peer_parts[i]
    send_buff = send_data[i]
    recv_buff = recv_data[i]
    mesh.recv_reqs[i] = MPI.Irecv!(recv_buff, peer_i, tag, mesh.comm)
    mesh.recv_waited[i] = false
    
    if !(mesh.send_waited[i])
      mesh.send_stats[i] = MPI.Wait!(mesh.send_reqs[i])
      mesh.send_waited[i] = true
    end
    mesh.send_reqs[i] = MPI.Isend(send_buff, peer_i, tag, mesh.comm)
    mesh.send_waited[i] = false
  end

  if wait
    mesh.recv_stats = MPI.Waitall!(mesh.recv_reqs)
    fill!(mesh.recv_waited, true)
    mesh.send_stats = MPI.Waitall!(mesh.send_reqs)
    fill!(mesh.send_waited, true)
  end

  return nothing
end

@doc """
### Utils.exchangeElementData

  This function posts the sends and receives for the exchange of ghost 
  element data.  It waits for the previous send request to finish before
  posting the new send.  It does not wait for the previous receive.

  Inputs:
    mesh: a mesh object
    opts: options dictonary
    q: the 3D array of data, from which the data for the element to be 
       sent to other processes will be picked out

  Inputs/Outputs:
    send_buff: array of arrays to copy the send data into.  There must be 
               mesh.npeers arrays, each size(q, 1) x size(q, 2) x number of
               elements on the local side of the interface
    recv_buff: array of arrays to put the received data into.  There must be
               mesh.npeers array, each size(q,1 ) x size(q, 2) x number of
               elements on the other side of the interface

  Keyword Args
    tag=TAG_ELEMENT: MPI Tag to use
    wait=false: whether not wait for all communications to finish before
                exiting

  Aliasing: no aliasing allowed
"""->
function exchangeElementData{T, N}(mesh::AbstractMesh, opts, q::Abstract3DArray,
                                   send_buff::Array{Array{T, N}, 1},
                                   recv_buff::Array{Array{T, N}, 1}, 
                                   f::IO=STDOUT;
                                   tag=TAG_ELEMENT, wait=false)

#  println(f, "----- Entered exchangeElementData -----")
  # post recieves
  for i=1:mesh.npeers
    peer_i = mesh.peer_parts[i]
    recv_buff_i = recv_buff[i]
    mesh.recv_reqs[i] = MPI.Irecv!(recv_buff_i, peer_i, tag, mesh.comm)
    mesh.recv_waited[i] = false
  end

  npeers = mesh.npeers
  val = sum(mesh.send_waited)
  if val != mesh.npeers && val != 0  # either all have been waited on or none
    throw(ErrorException("send requests partially waited on: $val of $npeers"))
  end

  for i=1:mesh.npeers
    # wait for these in order because doing the waitany trick doesn't work
    # these should have completed long ago, so it shouldn't be a performance
    # problem

    # the waitany trick doesn't work because this loop posts new sends, reusing
    # the same array of MPI_Requests.

    idx = i
    if !mesh.send_waited[i]
      MPI.Wait!(mesh.send_reqs[i])
      mesh.send_waited[idx] = true
    end

    # copy data into send buffer
    local_els = mesh.local_element_lists[idx]
    send_buff_i = send_buff[idx]
    for j=1:length(local_els)
      el_j = local_els[j]
      for k=1:size(q, 2)
        for p=1:size(q, 1)
          send_buff_i[p, k, j] = q[p, k, el_j]
        end
      end
    end

    # send it
    peer_i = mesh.peer_parts[idx]
    mesh.send_reqs[idx] = MPI.Isend(send_buff_i, peer_i, tag, mesh.comm)
    mesh.send_waited[idx] = false
  end

  if wait
    mesh.recv_stats = MPI.Waitall!(mesh.recv_reqs)
    fill!(mesh.recv_waited, true)
    mesh.send_stats = MPI.Waitall!(mesh.send_reqs)
    fill!(mesh.send_waited, true)
  end


  return nothing
end


@doc """
### Utils.verifyCommunication

  This function checks the data provided by the Status object to verify a 
  communication completed successfully.  The sender's rank and the number of
  elements is checked agains the expected sender and the buffer size

  Inputs:
    mesh: an AbstractMesh
    opts: options dictonary
    buff: the buffer
    peer: the expected sender
    stat: the Status object
"""->
function verifyCommunication{T}(mesh::AbstractMesh, opts, buff::Array{T}, peer::Integer, stat::MPI.Status)
# verify a communication occured correctly by checking the fields of the 
# Status object
# if the Status came from a send, then peer should be comm_rank ?
  sender = MPI.Get_source(stat)
  @assert sender == peer

  ndata = MPI.Get_count(stat, T)
  @assert ndata == length(buff)

  return nothing
end

# deprecated
@doc """
### Utils.getSendData

  This function interpolates the data that will be sent to peer processes
  and puts it into a buffer array.  This function waits for the Request
  to finish before overwriting the buffer.

  Inputs:
    mesh:: an AbstractDGMesh
    opts: options dictonary
    q: a 3D array numDofPerNode x numNodesPerElement x numEl holding the 
        original copy of the data
    interfaces: an array of Boundary types (not Interface) that describe
                the element and face owned by this process
    req: an MPI.Request object corresponding to the previous send using this
         buffer
    req_waited: a Bool indicating whether the request was waited on already

  Inputs/Outputs:
    buff: array numDofPerNode x numNodesPerFace x length(interfaces) to put
         the resulting data into

  Output:
    req_waited: a Bool indicating the request was waited on (should always
                be true)

  Aliasing restrictions: all bets are off if q and buff alias
"""->
function getSendData{T}(mesh::AbstractDGMesh, opts, q::AbstractArray{T, 3}, 
                    interfaces::Array{Boundary, 1}, buff::AbstractArray{T, 3},
                    req::MPI.Request, req_waited::Bool)

# get data out of the q array (which must be ndofPerNode x numNodesPerElement 
# x numEl), interpolate it to the shared interfaces, and store to buff
  @assert mesh.isInterpolated
  # wait for the previous send to complete before overwritting the
  # buffer

  if !req_waited
    MPI.Wait!(req)
    req_waited = true
  end
#  MPI.Waitall!(mesh.send_reqs)
  boundaryinterpolate!(mesh.sbpface, interfaces, q, buff)

  return req_waited
end

"""
  This function populates the send buffer from eqn.q for 
  opts["parallle_data"]  == "face"

  Inputs:
    mesh: a mesh
    sbp: an SBP operator
    eqn: an AbstractSolutionData
    opts: options dictonary

  Inputs/Outputs:
    data: a SharedFaceData.  data.q_send will be overwritten
"""
function getSendDataFace(mesh::AbstractMesh, sbp::AbstractSBP,
                         eqn::AbstractSolutionData, opts, data::SharedFaceData)


  idx = data.peeridx
  bndryfaces = data.bndries_local
  boundaryinterpolate!(mesh.sbpface, bndryfaces, eqn.q, data.q_send)

  return nothing
end

"""
  This function populates the send buffer from eqn.q for 
  opts["parallle_data"]  == "element"

  Inputs:
    mesh: a mesh
    sbp: an SBP operator
    eqn: an AbstractSolutionData
    opts: options dictonary

  Inputs/Outputs:
    data: a SharedFaceData.  data.q_send will be overwritten
"""
function getSendDataElement(mesh::AbstractMesh, sbp::AbstractSBP,
                         eqn::AbstractSolutionData, opts, data::SharedFaceData)

  # copy data into send buffer
  idx = data.peeridx
  local_els = mesh.local_element_lists[idx]
  send_buff = data_i.q_send
  for j=1:length(local_els)
    el_j = local_els[j]
    for k=1:size(q, 2)
      for p=1:size(q, 1)
        send_buff[p, k, j] = eqn.q[p, k, el_j]
      end
    end
  end

  return nothing
end


@doc """
### Utils.mpi_master

  This macro introduces an if statement that causes the expression to be 
  executed only if the variable myrank is equal to zero.  myrank must exist
  in the scope of the caller

"""->
macro mpi_master(ex)
  return quote
#    println("myrank = ", esc(myrank))
    if $(esc(:(myrank == 0)))
      $(esc(ex))
    end
  end
end

@doc """
### Utils.time_all 

  This macro returns the value produced by the expression as well as 
  the execution time, the GC time, and the amount of memory allocated
"""->
macro time_all(ex)
  quote
    local stats = Base.gc_num()
    local elapsedtime = time_ns()
    local val = $(esc(ex))
    elapsedtime = time_ns() - elapsedtime
    local diff = Base.GC_Diff(Base.gc_num(), stats)
    (val, elapsedtime/1e9, diff.total_time/1e9, diff.allocd)
  end
end

function print_time_all(f, t_elapsed, t_gc, alloc)
    println(f, t_elapsed, " seconds, ", t_gc, " GC seconds, ", alloc, " bytes allocated")
end
